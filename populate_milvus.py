import torch
from sentence_transformers import SentenceTransformer
import os
import re

# Import cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t tá»« cÃ¡c file khÃ¡c
from milvus import get_or_create_collection
from config import PDF_PATH, EMBEDDING_MODEL_NAME, EMBEDDING_DIM, COLLECTION_NAME
from export_md import convert_to_markdown

import nltk

def get_embedding_model():
    """
    Khá»Ÿi táº¡o vÃ  tráº£ vá» model embedding, Æ°u tiÃªn sá»­ dá»¥ng GPU náº¿u cÃ³.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ¤– Äang táº£i model embedding '{EMBEDDING_MODEL_NAME}' lÃªn '{device}'...")
    
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
        print("   -> âœ… Model Ä‘Ã£ táº£i thÃ nh cÃ´ng!")
        return model
    except Exception as e:
        print(f"   -> âŒ Lá»—i khi táº£i model: {e}")
        return None

def download_nltk_punkt():
    """
    Kiá»ƒm tra vÃ  táº£i vá» cÃ¡c gÃ³i tokenizer cáº§n thiáº¿t cá»§a NLTK.
    """
    try:
        # Pháº£i kiá»ƒm tra cáº£ hai tÃ i nguyÃªn, náº¿u má»™t trong hai thiáº¿u, sáº½ gÃ¢y ra LookupError
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("   -> ğŸ“– Äang táº£i cÃ¡c gÃ³i tokenizer cáº§n thiáº¿t cho NLTK ('punkt', 'punkt_tab')...")
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)
        print("   -> âœ… Táº£i tokenizer hoÃ n táº¥t.")

def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap_sentences: int = 2):
    """
    Chia vÄƒn báº£n thÃ nh cÃ¡c chunk má»™t cÃ¡ch thÃ´ng minh, dá»±a trÃªn ranh giá»›i cÃ¢u.
    Sá»­ dá»¥ng NLTK Ä‘á»ƒ tÃ¡ch cÃ¢u vÃ  nhÃ³m chÃºng láº¡i, vá»›i sá»± gá»‘i Ä‘áº§u giá»¯a cÃ¡c chunk.
    
    Args:
        text (str): Äoáº¡n vÄƒn báº£n cáº§n chia.
        chunk_size (int): KÃ­ch thÆ°á»›c tá»‘i Ä‘a (sá»‘ kÃ½ tá»±) cá»§a má»—i chunk.
        chunk_overlap_sentences (int): Sá»‘ cÃ¢u gá»‘i Ä‘áº§u giá»¯a cÃ¡c chunk liÃªn tiáº¿p.
    """
    if not text:
        return []

    # 1. TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
    sentences = nltk.sent_tokenize(text)
    
    # 2. NhÃ³m cÃ¡c cÃ¢u thÃ nh cÃ¡c chunk
    chunks = []
    current_chunk_sentences = []
    current_length = 0
    
    for i, sentence in enumerate(sentences):
        sentence_length = len(sentence)
        
        # Náº¿u thÃªm cÃ¢u nÃ y vÃ o sáº½ quÃ¡ dÃ i, hÃ£y hoÃ n thÃ nh chunk hiá»‡n táº¡i
        if current_length + sentence_length > chunk_size and current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))
            
            # Báº¯t Ä‘áº§u chunk má»›i vá»›i sá»± gá»‘i Ä‘áº§u
            # Láº¥y N cÃ¢u cuá»‘i tá»« chunk vá»«a hoÃ n thÃ nh Ä‘á»ƒ lÃ m pháº§n gá»‘i Ä‘áº§u
            overlap_start_index = max(0, len(current_chunk_sentences) - chunk_overlap_sentences)
            current_chunk_sentences = current_chunk_sentences[overlap_start_index:]
            # Cáº§n tÃ­nh láº¡i Ä‘á»™ dÃ i cá»§a chunk má»›i sau khi cÃ³ overlap
            current_length = len(" ".join(current_chunk_sentences))

        # ThÃªm cÃ¢u vÃ o chunk hiá»‡n táº¡i (dÃ¹ lÃ  chunk má»›i hay cÅ©)
        current_chunk_sentences.append(sentence)
        current_length += sentence_length

    # Äá»«ng quÃªn chunk cuá»‘i cÃ¹ng
    if current_chunk_sentences:
        chunks.append(" ".join(current_chunk_sentences))
        
    return chunks

def populate_database():
    """
    HÃ m chÃ­nh: Tá»± Ä‘á»™ng táº¡o file Markdown náº¿u cáº§n, Ä‘á»c ná»™i dung,
    táº¡o embedding, vÃ  lÆ°u vÃ o Milvus.
    """
    download_nltk_punkt() # Äáº£m báº£o NLTK Ä‘Ã£ sáºµn sÃ ng
    print("--- Báº®T Äáº¦U QUÃ TRÃŒNH Äá»’NG Bá»˜ Dá»® LIá»†U VÃ€O MILVUS ---")

    # --- BÆ°á»›c 1: Äáº£m báº£o file Markdown tá»“n táº¡i ---
    print("\n--- BÆ°á»›c 1: Chuáº©n bá»‹ file Markdown nguá»“n ---")
    md_filename = os.path.splitext(os.path.basename(PDF_PATH))[0] + ".md"
    md_filepath = os.path.join(os.path.dirname(PDF_PATH), md_filename)

    if not os.path.exists(md_filepath):
        print(f"   -> âš ï¸ File '{md_filename}' khÃ´ng tá»“n táº¡i. Tá»± Ä‘á»™ng táº¡o má»›i tá»« PDF...")
        markdown_content = convert_to_markdown(PDF_PATH)
        try:
            with open(md_filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            print(f"   -> âœ… ÄÃ£ táº¡o vÃ  lÆ°u file '{md_filename}' thÃ nh cÃ´ng.")
        except Exception as e:
            print(f"   -> âŒ Lá»—i khi lÆ°u file Markdown: {e}")
            return
    else:
        print(f"   -> âœ… ÄÃ£ tÃ¬m tháº¥y file '{md_filename}'.")

    # --- BÆ°á»›c 2: Äá»c vÃ  xá»­ lÃ½ file Markdown ---
    print(f"\n--- BÆ°á»›c 2: Äá»c vÃ  xá»­ lÃ½ file: {md_filename} ---")
    try:
        with open(md_filepath, "r", encoding="utf-8") as f:
            full_content = f.read()
    except Exception as e:
        print(f"   -> âŒ Lá»—i khi Ä‘á»c file Markdown: {e}")
        return

    # --- BÆ°á»›c 3: Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t ---
    model = get_embedding_model()
    if not model:
        return

    print("\n--- BÆ°á»›c 3: Chuáº©n bá»‹ collection trÃªn Milvus ---")
    collection = get_or_create_collection(COLLECTION_NAME, dim=EMBEDDING_DIM, recreate=True)

    # --- BÆ°á»›c 4: PhÃ¢n tÃ¡ch ná»™i dung vÃ  táº¡o chunks ---
    print("\n--- BÆ°á»›c 4: PhÃ¢n tÃ¡ch ná»™i dung vÃ  táº¡o chunks ---")
    all_chunks = []
    metadata = []

    # TÃ¡ch file MD thÃ nh cÃ¡c trang dá»±a trÃªn marker
    # Regex Ä‘á»ƒ tÃ¬m "--- Trang X (Nguá»“n: Y) ---"
    page_splits = re.split(r'--- Trang (\d+) \(Nguá»“n: [^)]+\) ---', full_content)

    # Bá» pháº§n Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  rá»—ng hoáº·c lÃ  tiÃªu Ä‘á» chÃ­nh)
    content_parts = page_splits[1:]
    
    if not content_parts:
        print("   -> âš ï¸ KhÃ´ng tÃ¬m tháº¥y marker trang nÃ o trong file MD. Coi toÃ n bá»™ file lÃ  má»™t trang.")
        page_chunks = chunk_text(full_content)
        all_chunks.extend(page_chunks)
        for _ in page_chunks:
            metadata.append({"pdf_source": os.path.basename(PDF_PATH), "page": 1})
    else:
        # GhÃ©p cáº·p [sá»‘ trang, ná»™i dung]
        for i in range(0, len(content_parts), 2):
            page_num = int(content_parts[i])
            page_content = content_parts[i+1].strip()
            
            if not page_content:
                continue

            page_chunks = chunk_text(page_content)
            all_chunks.extend(page_chunks)
            
            # GÃ¡n cÃ¹ng má»™t sá»‘ trang cho táº¥t cáº£ cÃ¡c chunk cá»§a trang Ä‘Ã³
            for _ in page_chunks:
                metadata.append({
                    "pdf_source": os.path.basename(PDF_PATH),
                    "page": page_num
                })
        print(f"   -> ÄÃ£ xá»­ lÃ½ vÃ  phÃ¢n tÃ¡ch Ä‘Æ°á»£c {len(set(m['page'] for m in metadata))} trang.")

    if not all_chunks:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n vÄƒn báº£n nÃ o Ä‘á»ƒ xá»­ lÃ½.")
        return
        
    print(f"   -> Tá»•ng cá»™ng cÃ³ {len(all_chunks)} Ä‘oáº¡n vÄƒn báº£n cáº§n xá»­ lÃ½.")

    # --- BÆ°á»›c 5: Táº¡o embedding cho táº¥t cáº£ cÃ¡c chunks ---
    print("\n--- BÆ°á»›c 5: Táº¡o embeddings cho cÃ¡c Ä‘oáº¡n vÄƒn báº£n ---")
    embeddings = model.encode(all_chunks, show_progress_bar=True)
    print("   -> âœ… Táº¡o embedding hoÃ n táº¥t.")

    # --- BÆ°á»›c 6: Chuáº©n bá»‹ vÃ  lÆ°u dá»¯ liá»‡u vÃ o Milvus ---
    print("\n--- BÆ°á»›c 6: LÆ°u dá»¯ liá»‡u vÃ o Milvus ---")
    entities = [
        embeddings,
        all_chunks,
        [meta['pdf_source'] for meta in metadata],
        [meta['page'] for meta in metadata]
    ]
    
    try:
        insert_result = collection.insert(entities)
        print(f"   -> âœ… ChÃ¨n thÃ nh cÃ´ng {insert_result.insert_count} vectors vÃ o Milvus.")
        
        print("   -> Äang flush collection...")
        collection.flush()
        print("   -> âœ… Flush hoÃ n táº¥t.")

    except Exception as e:
        print(f"   -> âŒ Lá»—i khi chÃ¨n dá»¯ liá»‡u vÃ o Milvus: {e}")

    print("\n--- QUÃ TRÃŒNH Äá»’NG Bá»˜ HOÃ€N Táº¤T ---")


if __name__ == "__main__":
    populate_database()
