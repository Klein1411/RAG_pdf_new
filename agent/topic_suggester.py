# coding: utf-8
"""
Topic Suggester - Äá» xuáº¥t chá»§ Ä‘á» tá»« tÃ i liá»‡u PDF
GiÃºp user biáº¿t cÃ³ thá»ƒ há»i gÃ¬ khi khÃ´ng biáº¿t há»i gÃ¬ hoáº·c khÃ´ng tÃ¬m tháº¥y thÃ´ng tin

TÃ­nh nÄƒng:
- Äá»c metadata tá»« collections
- TrÃ­ch xuáº¥t cÃ¡c chá»§ Ä‘á»/keywords tá»« tÃ i liá»‡u
- Táº¡o cÃ¢u há»i gá»£i Ã½ (suggestions)
- Cache táº¡m thá»i (xÃ³a khi táº¯t agent hoáº·c clear)
"""

import sys
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
import re
from collections import Counter, defaultdict

# ThÃªm project root vÃ o path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.logging_config import get_logger

logger = get_logger(__name__)


class TopicSuggester:
    """
    Táº¡o vÃ  quáº£n lÃ½ topic suggestions tá»« tÃ i liá»‡u
    Cache táº¡m thá»i - reset khi clear hoáº·c táº¯t agent
    """
    
    def __init__(self):
        # Cache topics theo collection
        self.topics_cache: Dict[str, List[Dict]] = {}
        
        # Keywords quan trá»ng (dÃ¹ng Ä‘á»ƒ trÃ­ch xuáº¥t topics)
        self.important_keywords = {
            # NLP/AI terms
            'rouge', 'bleu', 'metric', 'evaluation', 'summarization',
            'nlp', 'transformer', 'bert', 'gpt', 'model', 'training',
            'accuracy', 'precision', 'recall', 'f1', 'score',
            'dataset', 'benchmark', 'performance', 'neural', 'network',
            
            # Vietnamese NLP
            'tiáº¿ng viá»‡t', 'vietnamese', 'ngÃ´n ngá»¯', 'xá»­ lÃ½', 'phÃ¢n tÃ­ch',
            'mÃ´ hÃ¬nh', 'huáº¥n luyá»‡n', 'Ä‘Ã¡nh giÃ¡', 'dá»¯ liá»‡u',
            
            # Research terms
            'method', 'approach', 'algorithm', 'technique', 'framework',
            'phÆ°Æ¡ng phÃ¡p', 'thuáº­t toÃ¡n', 'ká»¹ thuáº­t', 'cÃ¡ch tiáº¿p cáº­n',
            'experiment', 'result', 'conclusion', 'thÃ­ nghiá»‡m', 'káº¿t quáº£',
            
            # General topics
            'introduction', 'background', 'literature', 'review',
            'giá»›i thiá»‡u', 'tá»•ng quan', 'nghiÃªn cá»©u', 'liÃªn quan',
        }
        
        # Stop words (tá»« bá» qua) - Má»ž Rá»˜NG
        self.stop_words = {
            # English common words
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were',
            'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must',
            'this', 'that', 'these', 'those', 'what', 'which', 'who', 'when',
            'where', 'why', 'how', 'can', 'not', 'no', 'yes',
            # Pronouns & common words
            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
            'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours', 'ours', 'theirs',
            'am', 'been', 'being', 'get', 'got', 'make', 'made', 'use', 'used',
            'see', 'seen', 'know', 'think', 'take', 'come', 'give', 'find', 'tell',
            'ask', 'work', 'seem', 'feel', 'try', 'leave', 'call', 'need', 'become',
            # Vietnamese
            'lÃ ', 'cá»§a', 'vÃ ', 'vá»›i', 'trong', 'Ä‘Æ°á»£c', 'cÃ³', 'nÃ y', 'Ä‘Ã³',
            'cÃ¡c', 'Ä‘á»ƒ', 'tá»«', 'má»™t', 'khÃ´ng', 'nhÆ°', 'vá»', 'cho', 'theo',
            'tÃ´i', 'báº¡n', 'anh', 'chá»‹', 'em', 'chÃºng', 'há»', 'ta',
            # Single characters & short words
            'tÃ³m', 'táº¯t', 'cáº§n', 'báº£n', 'Ä‘á»', 'má»¥c', 'sá»‘', 'vÃ­', 'dá»¥',
        }
        
        logger.info("TopicSuggester initialized")
    
    def extract_topics_from_collection(
        self, 
        collection_name: str,
        sample_texts: List[str],
        max_topics: int = 10
    ) -> List[Dict[str, Any]]:
        """
        TrÃ­ch xuáº¥t topics tá»« sample texts cá»§a collection
        Cáº£i thiá»‡n: láº¥y cáº£ n-grams (cá»¥m tá»«) khÃ´ng chá»‰ tá»« Ä‘Æ¡n
        
        Args:
            collection_name: TÃªn collection
            sample_texts: Danh sÃ¡ch text máº«u tá»« collection
            max_topics: Sá»‘ lÆ°á»£ng topics tá»‘i Ä‘a
            
        Returns:
            List of topics vá»›i format:
            {
                'keyword': str,
                'frequency': int,
                'questions': List[str]
            }
        """
        # TrÃ­ch xuáº¥t cáº£ unigrams vÃ  bigrams
        all_unigrams = []
        all_bigrams = []
        
        for text in sample_texts:
            # Láº¥y tá»« tá»« text (lowercase, remove punctuation)
            words = re.findall(r'\b[a-zA-ZÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]+\b', text.lower())
            
            # Unigrams (single words)
            all_unigrams.extend(words)
            
            # Bigrams (2-word phrases)
            for i in range(len(words) - 1):
                bigram = f"{words[i]} {words[i+1]}"
                all_bigrams.append(bigram)
        
        # Äáº¿m frequency
        unigram_counts = Counter(all_unigrams)
        bigram_counts = Counter(all_bigrams)
        
        # Filter unigrams: bá» stop words, chá»‰ giá»¯ tá»« dÃ i >= 4 kÃ½ tá»± (trÃ¡nh "tÃ³m", "táº¯t")
        filtered_unigrams = {
            word: count 
            for word, count in unigram_counts.items() 
            if word not in self.stop_words 
            and len(word) >= 4  # TÄƒng tá»« 3 lÃªn 4
            and count >= 2  # Xuáº¥t hiá»‡n Ã­t nháº¥t 2 láº§n
        }
        
        # Filter bigrams: bá» nhá»¯ng cá»¥m cÃ³ stop words
        filtered_bigrams = {}
        for bigram, count in bigram_counts.items():
            words = bigram.split()
            # Chá»‰ giá»¯ náº¿u cáº£ 2 tá»« Ä‘á»u khÃ´ng pháº£i stop word vÃ  count >= 2
            if (len(words) == 2 
                and all(w not in self.stop_words for w in words)
                and all(len(w) >= 3 for w in words)
                and count >= 2):
                filtered_bigrams[bigram] = count
        
        # Æ¯u tiÃªn important keywords (unigrams)
        prioritized_unigrams = {}
        for word, count in filtered_unigrams.items():
            if word in self.important_keywords:
                prioritized_unigrams[word] = count * 3  # Boost important words
            else:
                prioritized_unigrams[word] = count
        
        # Æ¯u tiÃªn bigrams chá»©a important keywords
        prioritized_bigrams = {}
        for bigram, count in filtered_bigrams.items():
            # Check if any word in bigram is important
            has_important = any(word in self.important_keywords for word in bigram.split())
            if has_important:
                prioritized_bigrams[bigram] = count * 2  # Boost bigrams with important words
            else:
                prioritized_bigrams[bigram] = count
        
        # Gá»™p unigrams vÃ  bigrams
        all_keywords = {**prioritized_unigrams, **prioritized_bigrams}
        
        # Láº¥y top keywords
        top_keywords = sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)[:max_topics]
        
        # Táº¡o topics vá»›i cÃ¢u há»i gá»£i Ã½
        topics = []
        for keyword, freq in top_keywords:
            topic = {
                'keyword': keyword,
                'frequency': freq,
                'questions': self._generate_questions(keyword)
            }
            topics.append(topic)
        
        logger.info(f"Extracted {len(topics)} topics from {collection_name}: {[t['keyword'] for t in topics]}")
        return topics
    
    def _generate_questions(self, keyword: str) -> List[str]:
        """
        Táº¡o cÃ¢u há»i gá»£i Ã½ tá»« keyword
        
        Args:
            keyword: Tá»« khÃ³a chá»§ Ä‘á» (cÃ³ thá»ƒ lÃ  bigram)
            
        Returns:
            Danh sÃ¡ch cÃ¢u há»i
        """
        # Check if keyword is a phrase (bigram)
        is_phrase = ' ' in keyword
        
        if is_phrase:
            # Bigram - cÃ¢u há»i phÃ¹ há»£p vá»›i cá»¥m tá»«
            questions = [
                f"Giáº£i thÃ­ch vá» {keyword}",
                f"{keyword.title()} lÃ  gÃ¬?",
                f"TÃ i liá»‡u cÃ³ Ä‘á» cáº­p gÃ¬ vá» {keyword}?",
                f"Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a {keyword}",
                f"PhÆ°Æ¡ng phÃ¡p {keyword} hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
            ]
        else:
            # Unigram - cÃ¢u há»i cho tá»« Ä‘Æ¡n
            questions = [
                f"{keyword.title()} lÃ  gÃ¬?",
                f"Giáº£i thÃ­ch vá» {keyword}",
                f"TÃ i liá»‡u cÃ³ Ä‘á» cáº­p gÃ¬ vá» {keyword}?",
                f"PhÆ°Æ¡ng phÃ¡p {keyword} hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
                f"Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a {keyword}",
            ]
        
        # Chá»n 2-3 cÃ¢u há»i ngáº«u nhiÃªn
        import random
        return random.sample(questions, min(3, len(questions)))
    
    def build_topics_from_collections(
        self,
        collection_names: List[str],
        sample_size: int = 50
    ) -> Dict[str, List[Dict]]:
        """
        XÃ¢y dá»±ng topics tá»« nhiá»u collections
        
        Args:
            collection_names: Danh sÃ¡ch tÃªn collection
            sample_size: Sá»‘ lÆ°á»£ng documents láº¥y máº«u tá»« má»—i collection
            
        Returns:
            Dict mapping collection_name -> topics
        """
        from pymilvus import Collection
        
        all_topics = {}
        
        for col_name in collection_names:
            try:
                # Load collection
                collection = Collection(col_name)
                collection.load()
                
                # Láº¥y sample documents
                # Query random documents
                results = collection.query(
                    expr="id >= 0",
                    output_fields=["text", "pdf_source"],  # Sá»­a tá»« 'source' thÃ nh 'pdf_source'
                    limit=sample_size
                )
                
                if not results:
                    logger.warning(f"No data in {col_name}")
                    continue
                
                # TrÃ­ch xuáº¥t texts
                sample_texts = [doc.get('text', '') for doc in results if doc.get('text')]
                
                # Extract topics
                topics = self.extract_topics_from_collection(col_name, sample_texts)
                all_topics[col_name] = topics
                
                # Cache
                self.topics_cache[col_name] = topics
                
                logger.info(f"âœ… Built topics for {col_name}: {len(topics)} topics")
                
            except Exception as e:
                logger.error(f"Error building topics for {col_name}: {e}")
                continue
        
        return all_topics
    
    def get_suggestions(
        self,
        collection_names: Optional[List[str]] = None,
        max_suggestions: int = 5
    ) -> List[str]:
        """
        Láº¥y danh sÃ¡ch cÃ¢u há»i gá»£i Ã½
        
        Args:
            collection_names: Danh sÃ¡ch collection cáº§n gá»£i Ã½ (None = táº¥t cáº£)
            max_suggestions: Sá»‘ lÆ°á»£ng gá»£i Ã½ tá»‘i Ä‘a
            
        Returns:
            Danh sÃ¡ch cÃ¢u há»i gá»£i Ã½
        """
        all_questions = []
        
        # Náº¿u chÆ°a cÃ³ cache, bÃ¡o lá»—i
        if not self.topics_cache:
            logger.warning("No topics cached. Call build_topics_from_collections first.")
            return [
                "HÃ£y há»i tÃ´i vá» ná»™i dung tÃ i liá»‡u",
                "Báº¡n muá»‘n tÃ¬m hiá»ƒu Ä‘iá»u gÃ¬?",
                "TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÃ¬m thÃ´ng tin tá»« PDF"
            ]
        
        # Láº¥y topics tá»« collections Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
        if collection_names:
            target_collections = collection_names
        else:
            target_collections = list(self.topics_cache.keys())
        
        # Tá»•ng há»£p cÃ¢u há»i tá»« topics
        for col_name in target_collections:
            if col_name in self.topics_cache:
                topics = self.topics_cache[col_name]
                for topic in topics:
                    all_questions.extend(topic['questions'])
        
        # Loáº¡i bá» trÃ¹ng láº·p vÃ  shuffle
        import random
        unique_questions = list(set(all_questions))
        random.shuffle(unique_questions)
        
        return unique_questions[:max_suggestions]
    
    def get_topic_summary(self, collection_names: Optional[List[str]] = None) -> str:
        """
        Táº¡o tÃ³m táº¯t cÃ¡c chá»§ Ä‘á» cÃ³ sáºµn
        
        Args:
            collection_names: Danh sÃ¡ch collection (None = táº¥t cáº£)
            
        Returns:
            String tÃ³m táº¯t cÃ¡c topics
        """
        if not self.topics_cache:
            return "ChÆ°a cÃ³ thÃ´ng tin vá» chá»§ Ä‘á». Vui lÃ²ng setup trÆ°á»›c."
        
        # Láº¥y collections
        if collection_names:
            target_collections = collection_names
        else:
            target_collections = list(self.topics_cache.keys())
        
        # Táº¡o summary
        summary_lines = ["ðŸ“š CÃC CHá»¦ Äá»€ CÃ“ Sáº´N TRONG TÃ€I LIá»†U:\n"]
        
        for col_name in target_collections:
            if col_name not in self.topics_cache:
                continue
            
            topics = self.topics_cache[col_name]
            if not topics:
                continue
            
            # Láº¥y tÃªn PDF tá»« collection name (loáº¡i bá» prefix)
            pdf_name = col_name.replace('collection_', '').replace('_', ' ')
            summary_lines.append(f"\nðŸ“„ {pdf_name}:")
            
            # Liá»‡t kÃª top 5 topics
            for i, topic in enumerate(topics[:5], 1):
                keyword = topic['keyword'].title()
                summary_lines.append(f"   {i}. {keyword}")
        
        return "\n".join(summary_lines)
    
    def clear_cache(self):
        """XÃ³a táº¥t cáº£ cache topics"""
        self.topics_cache.clear()
        logger.info("Topics cache cleared")
    
    def has_topics(self) -> bool:
        """Kiá»ƒm tra xem cÃ³ topics trong cache khÃ´ng"""
        return len(self.topics_cache) > 0


# Singleton instance
_suggester_instance = None

def get_topic_suggester() -> TopicSuggester:
    """Láº¥y instance cá»§a TopicSuggester (singleton)"""
    global _suggester_instance
    if _suggester_instance is None:
        _suggester_instance = TopicSuggester()
        logger.info("TopicSuggester instance created")
    return _suggester_instance


def reset_topic_suggester():
    """Reset singleton instance (dÃ¹ng khi cáº§n clear hoÃ n toÃ n)"""
    global _suggester_instance
    if _suggester_instance:
        _suggester_instance.clear_cache()
    _suggester_instance = None
    logger.info("TopicSuggester instance reset")


# Test function
def test_suggester():
    """Test TopicSuggester"""
    suggester = TopicSuggester()
    
    # Test vá»›i sample texts
    sample_texts = [
        "ROUGE is a metric for evaluating summarization quality.",
        "BLEU score measures machine translation quality.",
        "The transformer model revolutionized NLP tasks.",
        "BERT uses bidirectional training for language understanding.",
        "Evaluation metrics include precision, recall, and F1 score.",
    ]
    
    topics = suggester.extract_topics_from_collection(
        "test_collection",
        sample_texts
    )
    
    print("\n" + "="*70)
    print("TEST TOPIC SUGGESTER")
    print("="*70)
    
    print(f"\nExtracted {len(topics)} topics:")
    for topic in topics:
        print(f"\nðŸ”‘ Keyword: {topic['keyword']} (frequency: {topic['frequency']})")
        print("   Suggested questions:")
        for q in topic['questions']:
            print(f"   - {q}")
    
    # Cache vÃ  test suggestions
    suggester.topics_cache['test_collection'] = topics
    suggestions = suggester.get_suggestions(max_suggestions=5)
    
    print(f"\n\nðŸ’¡ Random suggestions ({len(suggestions)}):")
    for i, q in enumerate(suggestions, 1):
        print(f"{i}. {q}")


if __name__ == "__main__":
    test_suggester()
