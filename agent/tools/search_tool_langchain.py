# coding: utf-8
"""
Search Tool - Phi√™n b·∫£n LangChain
Tool t√¨m ki·∫øm trong nhi·ªÅu collection v·ªõi giao di·ªán LangChain Tool

Migration t·ª´ search_tool.py:
- Gi·ªØ nguy√™n core logic (SentenceTransformer + Milvus)
- Th√™m LangChain Tool decorator v√† interface
- T∆∞∆°ng th√≠ch v·ªõi LangGraph agents
"""

import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Annotated
import json

project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import torch
from pymilvus import Collection
from sentence_transformers import SentenceTransformer

# LangChain imports
from langchain_core.tools import tool
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field

from src.config import EMBEDDING_MODEL_NAME
from src.logging_config import get_logger

logger = get_logger(__name__)


# ============================================================================
# Pydantic schemas cho tool inputs
# ============================================================================

class SearchInput(BaseModel):
    """Input schema cho search_collections tool."""
    query: str = Field(description="C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm")
    collection_names: List[str] = Field(description="Danh s√°ch t√™n c√°c collection c·∫ßn search")
    top_k: int = Field(default=15, description="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ m·ªói collection")
    similarity_threshold: float = Field(default=0.15, description="Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu (0-1)")


class SearchSingleInput(BaseModel):
    """Input schema cho search_single_collection tool."""
    query: str = Field(description="C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm")
    collection_name: str = Field(description="T√™n collection c·∫ßn search")
    top_k: int = Field(default=15, description="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ")
    similarity_threshold: float = Field(default=0.15, description="Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu (0-1)")


# ============================================================================
# Core SearchTool class (gi·ªØ nguy√™n logic c≈©)
# ============================================================================

class SearchToolLangChain:
    """
    Vector search tool v·ªõi giao di·ªán LangChain.
    
    C√≥ th·ªÉ d√πng nh∆∞:
    1. Standalone class (backward compatible)
    2. LangChain tools qua get_langchain_tools()
    """
    
    def __init__(self, embedding_model: Optional[SentenceTransformer] = None):
        """
        Args:
            embedding_model: Model SentenceTransformer ƒë√£ load s·∫µn (t√πy ch·ªçn)
        """
        self.name = "search_tool_langchain"
        self.description = "T√¨m ki·∫øm trong nhi·ªÅu PDF collection b·∫±ng vector similarity"
        
        # D√πng model ƒë∆∞·ª£c truy·ªÅn v√†o ho·∫∑c load m·ªõi
        if embedding_model:
            self.embedding_model = embedding_model
        else:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME).to(device)
            logger.info(f"üîß ƒê√£ load embedding model tr√™n {device}")
    
    def search_multi_collections(
        self,
        query: str,
        collection_names: List[str],
        top_k: int = 15,
        similarity_threshold: float = 0.15
    ) -> List[Dict[str, Any]]:
        """
        T√¨m ki·∫øm trong nhi·ªÅu collections.
        
        Args:
            query: C√¢u h·ªèi t√¨m ki·∫øm
            collection_names: Danh s√°ch t√™n collections
            top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa m·ªói collection
            similarity_threshold: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu
            
        Returns:
            List k·∫øt qu·∫£ ƒë√£ sort theo score
        """
        logger.info(f"üîç ƒêang search trong {len(collection_names)} collections: {collection_names}")
        
        # Encode query
        query_vector = self.embedding_model.encode([query])[0]
        
        all_results = []
        
        for col_name in collection_names:
            try:
                # Load collection
                collection = Collection(col_name)
                collection.load()
                
                # Search parameters
                search_params = {
                    "metric_type": "L2",
                    "params": {"nprobe": 10}
                }
                
                # Execute search
                results = collection.search(
                    data=[query_vector.tolist()],
                    anns_field="embedding",
                    param=search_params,
                    limit=top_k,
                    output_fields=["text", "page", "pdf_source"]
                )
                
                # Process results
                for hit in results[0]:
                    score = 1.0 / (1.0 + hit.distance)  # Chuy·ªÉn L2 distance sang similarity
                    
                    if score >= similarity_threshold:
                        all_results.append({
                            'text': hit.entity.get('text'),
                            'page': hit.entity.get('page'),
                            'pdf_source': hit.entity.get('pdf_source'),
                            'collection': col_name,
                            'score': score,
                            'distance': hit.distance
                        })
                
                logger.debug(f"‚úÖ T√¨m th·∫•y {len([r for r in results[0] if 1.0/(1.0+r.distance) >= similarity_threshold])} k·∫øt qu·∫£ trong {col_name}")
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi search collection {col_name}: {e}")
                continue
        
        # Sort theo score gi·∫£m d·∫ßn
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"üìä T·ªïng s·ªë k·∫øt qu·∫£: {len(all_results)}")
        return all_results
    
    def search_single_collection(
        self,
        query: str,
        collection_name: str,
        top_k: int = 15,
        similarity_threshold: float = 0.15
    ) -> List[Dict[str, Any]]:
        """
        T√¨m ki·∫øm trong m·ªôt collection (convenience method).
        
        Args:
            query: C√¢u h·ªèi t√¨m ki·∫øm
            collection_name: T√™n collection
            top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
            similarity_threshold: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu
            
        Returns:
            List k·∫øt qu·∫£ ƒë√£ sort theo score
        """
        return self.search_multi_collections(
            query=query,
            collection_names=[collection_name],
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )
    
    def format_results_for_context(
        self,
        results: List[Dict[str, Any]],
        max_results: Optional[int] = None
    ) -> str:
        """
        Format search results th√†nh context string cho LLM.
        
        Args:
            results: List k·∫øt qu·∫£ t·ª´ search_multi_collections
            max_results: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t·ªëi ƒëa (None = t·∫•t c·∫£)
            
        Returns:
            Context string ƒë√£ format s·∫µn
        """
        if not results:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
        
        # Limit results if specified
        if max_results:
            results = results[:max_results]
        
        # Format m·ªói result th√†nh text block
        context_parts = []
        for i, result in enumerate(results, 1):
            text = result.get('text', '')
            source = result.get('pdf_source', 'Unknown')
            page = result.get('page', 'N/A')
            collection = result.get('collection', 'N/A')
            score = result.get('score', 0.0)
            
            context_parts.append(
                f"[{i}] (Score: {score:.3f})\n"
                f"Source: {source} (Page {page}, Collection: {collection})\n"
                f"{text}\n"
            )
        
        return "\n---\n".join(context_parts)
    
    def get_langchain_tools(self) -> List[BaseTool]:
        """
        L·∫•y LangChain tools ƒë·ªÉ d√πng trong agent.
        
        Returns:
            List ch·ª©a 2 LangChain tools
        """
        # T·∫°o tools v·ªõi quy·ªÅn truy c·∫≠p self
        tools = []
        
        # Tool 1: Search nhi·ªÅu collections
        @tool(args_schema=SearchInput)
        def search_collections(
            query: str,
            collection_names: List[str],
            top_k: int = 15,
            similarity_threshold: float = 0.15
        ) -> str:
            """Search for relevant documents across multiple PDF collections.
            
            Use this when you need to find information from specific collections.
            Returns top matching text chunks with their sources and scores.
            """
            results = self.search_multi_collections(
                query=query,
                collection_names=collection_names,
                top_k=top_k,
                similarity_threshold=similarity_threshold
            )
            
            # Format th√†nh JSON string cho LLM
            return json.dumps({
                "total_results": len(results),
                "results": results[:20]  # Gi·ªõi h·∫°n top 20 ƒë·ªÉ ti·∫øt ki·ªám context
            }, ensure_ascii=False, indent=2)
        
        # Tool 2: Search m·ªôt collection
        @tool(args_schema=SearchSingleInput)
        def search_single_collection(
            query: str,
            collection_name: str,
            top_k: int = 15,
            similarity_threshold: float = 0.15
        ) -> str:
            """Search for relevant documents in a single PDF collection.
            
            Use this when you know the specific collection to search.
            Faster than multi-collection search.
            """
            results = self.search_single_collection(
                query=query,
                collection_name=collection_name,
                top_k=top_k,
                similarity_threshold=similarity_threshold
            )
            
            # Format th√†nh JSON string cho LLM
            return json.dumps({
                "total_results": len(results),
                "results": results[:20]
            }, ensure_ascii=False, indent=2)
        
        tools.append(search_collections)
        tools.append(search_single_collection)
        
        return tools


# ============================================================================
# Standalone tool functions (ƒë·ªÉ d√πng ƒë·ªôc l·∫≠p, kh√¥ng c·∫ßn kh·ªüi t·∫°o class)
# ============================================================================

# Global instance (lazy loaded)
_global_search_tool: Optional[SearchToolLangChain] = None

def get_global_search_tool() -> SearchToolLangChain:
    """L·∫•y ho·∫∑c t·∫°o global SearchToolLangChain instance (singleton)."""
    global _global_search_tool
    if _global_search_tool is None:
        _global_search_tool = SearchToolLangChain()
    return _global_search_tool


@tool(args_schema=SearchInput)
def search_collections_tool(
    query: str,
    collection_names: List[str],
    top_k: int = 15,
    similarity_threshold: float = 0.15
) -> str:
    """Search for relevant documents across multiple PDF collections.
    
    Use this when you need to find information from multiple collections.
    Returns top matching text chunks with their sources and scores.
    
    Args:
        query: C√¢u h·ªèi ho·∫∑c query t√¨m ki·∫øm
        collection_names: Danh s√°ch t√™n c√°c collections c·∫ßn search
        top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa m·ªói collection (m·∫∑c ƒë·ªãnh 15)
        similarity_threshold: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu 0-1 (m·∫∑c ƒë·ªãnh 0.15)
    """
    tool = get_global_search_tool()
    results = tool.search_multi_collections(
        query=query,
        collection_names=collection_names,
        top_k=top_k,
        similarity_threshold=similarity_threshold
    )
    
    return json.dumps({
        "total_results": len(results),
        "results": results[:20]
    }, ensure_ascii=False, indent=2)


# ============================================================================
# Backward compatibility (t∆∞∆°ng th√≠ch v·ªõi code c≈©)
# ============================================================================

# Alias ƒë·ªÉ code c≈© v·∫´n ch·∫°y ƒë∆∞·ª£c
SearchTool = SearchToolLangChain


if __name__ == "__main__":
    """Script test nhanh"""
    print("=" * 70)
    print("üß™ Testing SearchToolLangChain")
    print("=" * 70)
    
    # Test 1: T·∫°o tool
    print("\n1Ô∏è‚É£ ƒêang t·∫°o SearchToolLangChain...")
    search_tool = SearchToolLangChain()
    print(f"   ‚úÖ ƒê√£ t·∫°o: {search_tool.name}")
    
    # Test 2: L·∫•y LangChain tools
    print("\n2Ô∏è‚É£ L·∫•y LangChain tools...")
    tools = search_tool.get_langchain_tools()
    print(f"   ‚úÖ C√≥ {len(tools)} tools:")
    for tool in tools:
        print(f"      - {tool.name}: {tool.description[:60]}...")
    
    # Test 3: Standalone tool
    print("\n3Ô∏è‚É£ Test standalone tool...")
    print(f"   ‚úÖ search_collections_tool: {search_collections_tool.name}")
    
    print("\n" + "=" * 70)
    print("‚úÖ T·∫•t c·∫£ tests passed!")
    print("=" * 70)
