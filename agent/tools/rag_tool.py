# coding: utf-8
"""
RAG Tool - Complete RAG Workflow
T√°ch to√†n b·ªô logic RAG t·ª´ agent.py ƒë·ªÉ d·ªÖ test v√† maintain

Ch·ª©c nƒÉng:
- Answer questions using RAG
- Split complex multi-part questions
- Multi-collection search
- Context formatting
- LLM answer generation
"""

import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import re

project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from agent.tools.search_tool import SearchTool
from src.logging_config import get_logger

logger = get_logger(__name__)


class RagTool:
    """
    Tool th·ª±c hi·ªán complete RAG workflow
    ƒê·ªôc l·∫≠p, testable, reusable
    """
    
    def __init__(
        self,
        search_tool: SearchTool,
        llm_client: Any,
        llm_type: str,
        ollama_model: Optional[str] = None
    ):
        """
        Kh·ªüi t·∫°o RagTool
        
        Args:
            search_tool: SearchTool instance ƒë·ªÉ search vectors
            llm_client: LLM client (Gemini ho·∫∑c Ollama)
            llm_type: 'gemini' ho·∫∑c 'ollama'
            ollama_model: T√™n model Ollama n·∫øu d√πng Ollama
        """
        self.name = "rag_tool"
        self.description = "RAG tool for answering questions from documents"
        
        self.search_tool = search_tool
        self.llm_client = llm_client
        self.llm_type = llm_type
        self.ollama_model = ollama_model
        
        logger.info(f"‚úÖ RagTool initialized with LLM type: {llm_type}")
    
    def answer_question(
        self,
        question: str,
        collection_names: List[str],
        conversation_history: Optional[List[Dict]] = None,
        top_k: int = 15
    ) -> Dict[str, Any]:
        """
        Complete RAG workflow ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi
        
        Args:
            question: C√¢u h·ªèi t·ª´ user
            collection_names: List c√°c collection c·∫ßn search
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i (optional)
            top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
            
        Returns:
            Dict v·ªõi keys:
            - 'success': bool
            - 'answer': str (c√¢u tr·∫£ l·ªùi)
            - 'sources': List[Dict] (ngu·ªìn tr√≠ch d·∫´n)
            - 'search_results': List[Dict] (k·∫øt qu·∫£ search)
            - 'error': str (n·∫øu c√≥ l·ªói)
        """
        try:
            logger.info(f"RAG: Answering question in {len(collection_names)} collections")
            
            # B∆Ø·ªöC 1: Ki·ªÉm tra c√¢u h·ªèi c√≥ ph·ª©c t·∫°p kh√¥ng (multi-part)
            sub_questions = self.split_complex_question(question)
            
            if len(sub_questions) > 1:
                # C√¢u h·ªèi ph·ª©c t·∫°p ‚Üí x·ª≠ l√Ω t·ª´ng ph·∫ßn
                logger.info(f"üìù Ph√°t hi·ªán {len(sub_questions)} c√¢u h·ªèi con")
                return self._handle_complex_question(
                    sub_questions,
                    collection_names,
                    conversation_history,
                    top_k
                )
            else:
                # C√¢u h·ªèi ƒë∆°n gi·∫£n
                return self._handle_simple_question(
                    question,
                    collection_names,
                    conversation_history,
                    top_k
                )
        
        except Exception as e:
            logger.error(f"‚ùå RAG error: {e}", exc_info=True)
            return {
                'success': False,
                'answer': f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
                'sources': [],
                'search_results': [],
                'error': str(e)
            }
    
    def _handle_simple_question(
        self,
        question: str,
        collection_names: List[str],
        conversation_history: Optional[List[Dict]],
        top_k: int
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω c√¢u h·ªèi ƒë∆°n gi·∫£n"""
        
        # B∆Ø·ªöC 2: Search trong collections
        search_results = self.search_tool.search_multi_collections(
            query=question,
            collection_names=collection_names,
            top_k=top_k,
            similarity_threshold=0.10
        )
        
        if not search_results:
            return {
                'success': True,
                'answer': "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu.",
                'sources': [],
                'search_results': []
            }
        
        # B∆Ø·ªöC 3: Format context cho LLM
        context = self.search_tool.format_results_for_context(
            search_results,
            max_results=top_k
        )
        
        # B∆Ø·ªöC 4: Generate answer v·ªõi LLM
        answer = self._generate_answer(
            question=question,
            context=context,
            conversation_history=conversation_history
        )
        
        # B∆Ø·ªöC 5: Extract sources
        sources = self._extract_sources(search_results)
        
        return {
            'success': True,
            'answer': answer,
            'sources': sources,
            'search_results': search_results
        }
    
    def _handle_complex_question(
        self,
        sub_questions: List[str],
        collection_names: List[str],
        conversation_history: Optional[List[Dict]],
        top_k: int
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p (multi-part)"""
        
        all_answers = []
        all_sources = []
        all_search_results = []
        
        # Tr·∫£ l·ªùi t·ª´ng c√¢u h·ªèi con
        for i, sub_q in enumerate(sub_questions, 1):
            logger.info(f"  [{i}/{len(sub_questions)}] {sub_q}")
            
            result = self._handle_simple_question(
                question=sub_q,
                collection_names=collection_names,
                conversation_history=conversation_history,
                top_k=top_k // len(sub_questions)  # Chia ƒë·ªÅu top_k
            )
            
            if result['success']:
                all_answers.append(f"**{i}. {sub_q}**\n{result['answer']}")
                all_sources.extend(result['sources'])
                all_search_results.extend(result['search_results'])
        
        # Combine answers
        combined_answer = "\n\n".join(all_answers)
        
        # Deduplicate sources
        unique_sources = self._deduplicate_sources(all_sources)
        
        return {
            'success': True,
            'answer': combined_answer,
            'sources': unique_sources,
            'search_results': all_search_results
        }
    
    def split_complex_question(self, question: str) -> List[str]:
        """
        T√°ch c√¢u h·ªèi ph·ª©c t·∫°p th√†nh c√°c c√¢u h·ªèi con
        
        Args:
            question: C√¢u h·ªèi g·ªëc
            
        Returns:
            List c√°c c√¢u h·ªèi con (n·∫øu l√† c√¢u h·ªèi ph·ª©c t·∫°p)
            ho·∫∑c [question] n·∫øu l√† c√¢u h·ªèi ƒë∆°n gi·∫£n
        """
        # Patterns cho complex questions
        patterns = [
            r'(\d+[\.\)])',  # Numbered: 1. 2. ho·∫∑c 1) 2)
            r'([a-z][\.\)])',  # Lettered: a. b. ho·∫∑c a) b)
            r'\band\b',  # "and" separator
            r'\bv√†\b',  # "v√†" separator (Vietnamese)
        ]
        
        # Check patterns
        for pattern in patterns[:2]:  # Ch·ªâ check numbered/lettered
            matches = re.findall(pattern, question)
            if len(matches) >= 2:
                # C√≥ pattern numbered/lettered
                parts = re.split(pattern, question)
                
                sub_questions = []
                for part in parts:
                    cleaned = part.strip()
                    # B·ªè s·ªë/ch·ªØ c√°i ·ªü ƒë·∫ßu
                    cleaned = re.sub(r'^[\d\w][\.\)]\s*', '', cleaned)
                    
                    if len(cleaned) > 10:  # C√¢u h·ªèi t·ªëi thi·ªÉu 10 k√Ω t·ª±
                        sub_questions.append(cleaned)
                
                if len(sub_questions) >= 2:
                    logger.debug(f"Split into {len(sub_questions)} sub-questions")
                    return sub_questions
        
        # Check "and"/"v√†" separator
        if ' and ' in question.lower() or ' v√† ' in question.lower():
            # T√°ch b·∫±ng "and" ho·∫∑c "v√†"
            separator = ' and ' if ' and ' in question.lower() else ' v√† '
            parts = question.split(separator)
            
            if len(parts) == 2:
                # Ch·ªâ split n·∫øu c·∫£ 2 ph·∫ßn ƒë·ªÅu d√†i ƒë·ªß
                if all(len(p.strip()) > 10 for p in parts):
                    sub_questions = [p.strip() for p in parts]
                    logger.debug(f"Split by '{separator}': {len(sub_questions)} parts")
                    return sub_questions
        
        # Kh√¥ng ph·∫£i c√¢u h·ªèi ph·ª©c t·∫°p
        return [question]
    
    def _generate_answer(
        self,
        question: str,
        context: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        Generate answer s·ª≠ d·ª•ng LLM
        
        Args:
            question: C√¢u h·ªèi
            context: Context t·ª´ search results
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i
            
        Returns:
            Answer string
        """
        # Build prompt
        prompt = f"""Based on the following information, answer the question accurately.

Context from documents:
{context}

Question: {question}

Instructions:
- Answer based ONLY on the provided context
- Be concise and clear
- If context doesn't contain relevant info, say "I cannot find information about this"
- Use the same language as the question (Vietnamese or English)

Answer:"""
        
        try:
            if self.llm_type == 'gemini':
                # Gemini
                answer = self.llm_client.generate_content(prompt)
                return answer.strip() if answer else "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."
            
            elif self.llm_type == 'ollama':
                # Ollama
                response = self.llm_client.chat(
                    model=self.ollama_model,
                    messages=[{'role': 'user', 'content': prompt}]
                )
                answer = response['message']['content'].strip()
                return answer
            
            else:
                return f"Unsupported LLM type: {self.llm_type}"
        
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return f"Xin l·ªói, g·∫∑p l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}"
    
    def _extract_sources(self, search_results: List[Dict]) -> List[Dict]:
        """
        Extract unique sources t·ª´ search results
        
        Args:
            search_results: List c√°c search results
            
        Returns:
            List unique sources v·ªõi format:
            [{'pdf': str, 'page': int, 'collection': str}, ...]
        """
        sources = []
        seen = set()
        
        for result in search_results:
            key = (result.get('source', ''), result.get('page', 0))
            
            if key not in seen:
                seen.add(key)
                sources.append({
                    'pdf': result.get('source', 'unknown'),
                    'page': result.get('page', 0),
                    'collection': result.get('collection', 'unknown')
                })
        
        return sources
    
    def _deduplicate_sources(self, sources: List[Dict]) -> List[Dict]:
        """Lo·∫°i b·ªè sources tr√πng l·∫∑p"""
        unique_sources = []
        seen = set()
        
        for source in sources:
            key = (source.get('pdf', ''), source.get('page', 0))
            if key not in seen:
                seen.add(key)
                unique_sources.append(source)
        
        return unique_sources


# Singleton instance
_rag_tool = None

def get_rag_tool(
    search_tool: SearchTool,
    llm_client: Any,
    llm_type: str,
    ollama_model: Optional[str] = None
) -> RagTool:
    """
    Get ho·∫∑c create RagTool instance
    
    Note: Kh√¥ng d√πng singleton v√¨ c·∫ßn pass dynamic params
    """
    return RagTool(
        search_tool=search_tool,
        llm_client=llm_client,
        llm_type=llm_type,
        ollama_model=ollama_model
    )
