"""
RAG Tool - Tool Ä‘á»ƒ Agent gá»i vÃ o RAG system

Tool nÃ y wrap cÃ¡c chá»©c nÄƒng cá»§a qa_app.py Ä‘á»ƒ Agent cÃ³ thá»ƒ sá»­ dá»¥ng.
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional

# ThÃªm thÆ° má»¥c gá»‘c project vÃ o sys.path
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from agent.config import (
    RAG_TOOL_NAME,
    RAG_TOOL_DESCRIPTION,
    RAG_MAX_RESULTS,
    RAG_SIMILARITY_THRESHOLD
)

# Import tá»« src
import torch
from sentence_transformers import SentenceTransformer
from src.config import EMBEDDING_MODEL_NAME, COLLECTION_NAME
from src.milvus import get_or_create_collection
from src.llm_handler import initialize_and_select_llm, generate_answer_with_fallback
from src.logging_config import get_logger

logger = get_logger(__name__)


class RAGTool:
    """
    Tool cho Agent Ä‘á»ƒ search vÃ  truy váº¥n thÃ´ng tin tá»« RAG system.
    
    Attributes:
        name: TÃªn cá»§a tool
        description: MÃ´ táº£ chá»©c nÄƒng
        embedding_model: Model Ä‘á»ƒ encode queries
        collection: Milvus collection
        llm_client: LLM client Ä‘á»ƒ generate answers
    """
    
    def __init__(self):
        """Khá»Ÿi táº¡o RAG Tool vá»›i cÃ¡c dependencies cáº§n thiáº¿t."""
        self.name = RAG_TOOL_NAME
        self.description = RAG_TOOL_DESCRIPTION
        
        logger.info(f"Äang khá»Ÿi táº¡o RAG Tool: {self.name}")
        
        # Khá»Ÿi táº¡o embedding model
        self.embedding_model = self._init_embedding_model()
        
        # Káº¿t ná»‘i Milvus
        self.collection = self._init_milvus_collection()
        
        # Khá»Ÿi táº¡o LLM client
        self.llm_client, self.model_choice, self.ollama_model_name = self._init_llm_client()
        
        logger.info("âœ… RAG Tool Ä‘Ã£ sáºµn sÃ ng")
    
    def _init_embedding_model(self) -> SentenceTransformer:
        """Khá»Ÿi táº¡o embedding model."""
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"Táº£i embedding model '{EMBEDDING_MODEL_NAME}' trÃªn {device}")
        
        try:
            model = SentenceTransformer(EMBEDDING_MODEL_NAME)
            model.to(device)
            return model
        except Exception as e:
            logger.error(f"Lá»—i khi táº£i embedding model: {e}")
            raise
    
    def _init_milvus_collection(self):
        """Káº¿t ná»‘i Ä‘áº¿n Milvus collection."""
        logger.info(f"Káº¿t ná»‘i Ä‘áº¿n Milvus collection: {COLLECTION_NAME}")
        
        try:
            collection = get_or_create_collection(
                collection_name=COLLECTION_NAME,
                dim=self.embedding_model.get_sentence_embedding_dimension(),
                recreate=False  # KhÃ´ng táº¡o má»›i, chá»‰ káº¿t ná»‘i
            )
            collection.load()
            return collection
        except Exception as e:
            logger.error(f"Lá»—i khi káº¿t ná»‘i Milvus: {e}")
            raise
    
    def _init_llm_client(self):
        """
        Khá»Ÿi táº¡o LLM client.
        
        Returns:
            Tuple of (llm_client, model_choice, ollama_model_name)
        """
        logger.info("Khá»Ÿi táº¡o LLM client cho RAG")
        
        try:
            model_choice, gemini_client, ollama_model_name = initialize_and_select_llm()
            
            # LÆ°u thÃ´ng tin model
            if model_choice == '1':
                self.llm_name = "Gemini"
                logger.info(f"Sá»­ dá»¥ng LLM: {self.llm_name}")
                return gemini_client, model_choice, ollama_model_name
            else:
                self.llm_name = f"Ollama ({ollama_model_name})"
                logger.info(f"Sá»­ dá»¥ng LLM: {self.llm_name}")
                return None, model_choice, ollama_model_name
                
        except Exception as e:
            logger.error(f"Lá»—i khi khá»Ÿi táº¡o LLM: {e}")
            raise
    
    def search(
        self, 
        query: str, 
        top_k: Optional[int] = None,
        threshold: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Search thÃ´ng tin trong RAG system.
        
        Args:
            query: CÃ¢u há»i/tá»« khÃ³a tÃ¬m kiáº¿m
            top_k: Sá»‘ káº¿t quáº£ tráº£ vá» (máº·c Ä‘á»‹nh tá»« config)
            threshold: NgÆ°á»¡ng similarity (máº·c Ä‘á»‹nh tá»« config)
            
        Returns:
            Dict chá»©a results vÃ  metadata
        """
        top_k = top_k or RAG_MAX_RESULTS
        threshold = threshold or RAG_SIMILARITY_THRESHOLD
        
        logger.info(f"ðŸ” Search query: '{query}' (top_k={top_k}, threshold={threshold})")
        
        try:
            # Encode query
            query_embedding = self.embedding_model.encode([query])[0].tolist()
            
            # Search trong Milvus (sá»­ dá»¥ng L2 distance thay vÃ¬ COSINE)
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
            results = self.collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                expr=None,
                output_fields=["text", "page", "pdf_source"]
            )
            
            # Parse results
            if not results or not results[0]:
                logger.warning("KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p")
                return {
                    "success": False,
                    "message": "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan",
                    "results": [],
                    "count": 0
                }
            
            # Filter by threshold vÃ  format results
            # Vá»›i L2 distance: cÃ ng nhá» cÃ ng giá»‘ng nhau
            # L2 distance range: [0, âˆž), nÃªn cÃ ng nhá» cÃ ng tá»‘t
            # Äá»ƒ dá»… hiá»ƒu: chuyá»ƒn L2 sang similarity score [0, 1]
            # similarity = 1 / (1 + distance)
            formatted_results = []
            
            logger.info(f"ðŸ“Š Distances from Milvus:")
            for hit in results[0]:
                # Chuyá»ƒn L2 distance thÃ nh similarity score
                similarity = 1.0 / (1.0 + hit.distance)
                logger.info(f"   - Distance: {hit.distance:.4f}, Similarity: {similarity:.4f}")
                
                # Chá»‰ láº¥y káº¿t quáº£ cÃ³ similarity >= threshold
                if similarity >= threshold:
                    formatted_results.append({
                        "text": hit.entity.get('text'),
                        "page": hit.entity.get('page'),
                        "source": hit.entity.get('pdf_source'),
                        "score": float(similarity)
                    })
            
            logger.info(f"âœ… TÃ¬m tháº¥y {len(formatted_results)} káº¿t quáº£")
            
            return {
                "success": True,
                "results": formatted_results,
                "count": len(formatted_results),
                "query": query
            }
            
        except Exception as e:
            logger.error(f"Lá»—i khi search: {e}")
            return {
                "success": False,
                "message": f"Lá»—i: {str(e)}",
                "results": [],
                "count": 0
            }
    
    def ask(
        self, 
        question: str,
        top_k: Optional[int] = None,
        return_context: bool = False
    ) -> Dict[str, Any]:
        """
        Há»i cÃ¢u há»i vÃ  nháº­n cÃ¢u tráº£ lá»i tá»« RAG system.
        
        Args:
            question: CÃ¢u há»i
            top_k: Sá»‘ context documents (máº·c Ä‘á»‹nh tá»« config)
            return_context: CÃ³ tráº£ vá» context khÃ´ng
            
        Returns:
            Dict chá»©a answer, sources vÃ  metadata
        """
        top_k = top_k or RAG_MAX_RESULTS
        
        logger.info(f"â“ Question: '{question}'")
        
        try:
            # Search context
            search_result = self.search(question, top_k=top_k)
            
            if not search_result["success"] or search_result["count"] == 0:
                return {
                    "success": False,
                    "answer": "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n.",
                    "sources": [],
                    "context": None
                }
            
            # Build context tá»« search results
            context_parts = []
            sources = []
            
            for i, result in enumerate(search_result["results"], 1):
                context_parts.append(f"[Äoáº¡n {i} - Trang {result['page']}]:\n{result['text']}")
                sources.append({
                    "page": result["page"],
                    "source": result["source"],
                    "score": result["score"]
                })
            
            context = "\n\n".join(context_parts)
            
            # Build prompt
            prompt = f"""Dá»±a trÃªn cÃ¡c thÃ´ng tin sau Ä‘Ã¢y tá»« tÃ i liá»‡u:

{context}

HÃ£y tráº£ lá»i cÃ¢u há»i: {question}

LÆ°u Ã½:
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p
- TrÃ­ch dáº«n sá»‘ trang khi cÃ³ thá»ƒ
- Náº¿u khÃ´ng Ä‘á»§ thÃ´ng tin, hÃ£y nÃ³i rÃµ
- Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch
"""
            
            # Generate answer
            logger.info("Äang generate cÃ¢u tráº£ lá»i...")
            answer = generate_answer_with_fallback(
                prompt,
                self.model_choice,
                self.llm_client,
                self.ollama_model_name
            )
            
            logger.info("âœ… ÄÃ£ generate cÃ¢u tráº£ lá»i")
            
            result = {
                "success": True,
                "answer": answer,
                "sources": sources,
                "query": question
            }
            
            if return_context:
                result["context"] = context
            
            return result
            
        except Exception as e:
            logger.error(f"Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {e}")
            return {
                "success": False,
                "answer": f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}",
                "sources": [],
                "context": None
            }
    
    def get_info(self) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin vá» tool."""
        return {
            "name": self.name,
            "description": self.description,
            "collection": COLLECTION_NAME,
            "embedding_model": EMBEDDING_MODEL_NAME,
            "max_results": RAG_MAX_RESULTS,
            "threshold": RAG_SIMILARITY_THRESHOLD
        }


# --- TOOL INTERFACE CHO AGENT ---

def create_rag_tool() -> RAGTool:
    """
    Factory function Ä‘á»ƒ táº¡o RAG tool.
    
    Returns:
        RAGTool instance
    """
    return RAGTool()


# --- TEST & DEMO ---
if __name__ == "__main__":
    print("=== Testing RAG Tool ===\n")
    
    # Khá»Ÿi táº¡o tool
    print("1. Khá»Ÿi táº¡o RAG Tool...")
    tool = create_rag_tool()
    print(f"   âœ… Tool info: {tool.get_info()}\n")
    
    # Test search
    print("2. Test search...")
    search_result = tool.search("chá»‰ sá»‘ ROUGE", top_k=15)  # Vietnamese query, top_k=15
    print(f"   Found: {search_result['count']} results")
    if search_result['results']:
        print(f"   Top result: {search_result['results'][0]['text'][:100]}...")
        print(f"   Score: {search_result['results'][0]['score']:.4f}\n")
    
    # Test ask
    print("\n3. Test ask...")
    answer_result = tool.ask("ROUGE lÃ  gÃ¬?", top_k=15)  # Vietnamese question, top_k=15
    print(f"   Answer: {answer_result['answer'][:200]}...")
    print(f"   Sources: {len(answer_result['sources'])} documents\n")
    
    print("=== Test completed ===")
