"""
RAG Tool - Tool ƒë·ªÉ Agent g·ªçi v√†o RAG system

Tool n√†y wrap c√°c ch·ª©c nƒÉng c·ªßa qa_app.py ƒë·ªÉ Agent c√≥ th·ªÉ s·ª≠ d·ª•ng.
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional

# Th√™m th∆∞ m·ª•c g·ªëc project v√†o sys.path
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from agent.config import (
    RAG_TOOL_NAME,
    RAG_TOOL_DESCRIPTION,
    RAG_MAX_RESULTS,
    RAG_SIMILARITY_THRESHOLD
)

# Import t·ª´ src
import torch
from sentence_transformers import SentenceTransformer
from src.config import EMBEDDING_MODEL_NAME, COLLECTION_NAME
from src.milvus import get_or_create_collection
from src.llm_handler import initialize_and_select_llm, generate_answer_with_fallback
from src.logging_config import get_logger

logger = get_logger(__name__)


class RAGTool:
    """
    Tool cho Agent ƒë·ªÉ search v√† truy v·∫•n th√¥ng tin t·ª´ RAG system.
    
    Attributes:
        name: T√™n c·ªßa tool
        description: M√¥ t·∫£ ch·ª©c nƒÉng
        embedding_model: Model ƒë·ªÉ encode queries
        collection: Milvus collection
        llm_client: LLM client ƒë·ªÉ generate answers
    """
    
    def __init__(self):
        """Kh·ªüi t·∫°o RAG Tool v·ªõi c√°c dependencies c·∫ßn thi·∫øt."""
        self.name = RAG_TOOL_NAME
        self.description = RAG_TOOL_DESCRIPTION
        
        logger.info(f"ƒêang kh·ªüi t·∫°o RAG Tool: {self.name}")
        
        # Kh·ªüi t·∫°o embedding model
        self.embedding_model = self._init_embedding_model()
        
        # K·∫øt n·ªëi Milvus
        self.collection = self._init_milvus_collection()
        
        # Kh·ªüi t·∫°o LLM client
        self.llm_client, self.model_choice, self.ollama_model_name = self._init_llm_client()
        
        logger.info("‚úÖ RAG Tool ƒë√£ s·∫µn s√†ng")
    
    def _init_embedding_model(self) -> SentenceTransformer:
        """Kh·ªüi t·∫°o embedding model."""
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"T·∫£i embedding model '{EMBEDDING_MODEL_NAME}' tr√™n {device}")
        
        try:
            model = SentenceTransformer(EMBEDDING_MODEL_NAME)
            model.to(device)
            return model
        except Exception as e:
            logger.error(f"L·ªói khi t·∫£i embedding model: {e}")
            raise
    
    def _init_milvus_collection(self):
        """K·∫øt n·ªëi ƒë·∫øn Milvus collection."""
        logger.info(f"K·∫øt n·ªëi ƒë·∫øn Milvus collection: {COLLECTION_NAME}")
        
        try:
            collection = get_or_create_collection(
                collection_name=COLLECTION_NAME,
                dim=self.embedding_model.get_sentence_embedding_dimension(),
                recreate=False  # Kh√¥ng t·∫°o m·ªõi, ch·ªâ k·∫øt n·ªëi
            )
            collection.load()
            return collection
        except Exception as e:
            logger.error(f"L·ªói khi k·∫øt n·ªëi Milvus: {e}")
            raise
    
    def _init_llm_client(self):
        """
        Kh·ªüi t·∫°o LLM client.
        
        Returns:
            Tuple of (llm_client, model_choice, ollama_model_name)
        """
        logger.info("Kh·ªüi t·∫°o LLM client cho RAG")
        
        try:
            model_choice, gemini_client, ollama_model_name = initialize_and_select_llm()
            
            # L∆∞u th√¥ng tin model
            if model_choice == '1':
                self.llm_name = "Gemini"
                logger.info(f"S·ª≠ d·ª•ng LLM: {self.llm_name}")
                return gemini_client, model_choice, ollama_model_name
            else:
                self.llm_name = f"Ollama ({ollama_model_name})"
                logger.info(f"S·ª≠ d·ª•ng LLM: {self.llm_name}")
                return None, model_choice, ollama_model_name
                
        except Exception as e:
            logger.error(f"L·ªói khi kh·ªüi t·∫°o LLM: {e}")
            raise
    
    def search(
        self, 
        query: str, 
        top_k: Optional[int] = None,
        threshold: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Search th√¥ng tin trong RAG system.
        
        Args:
            query: C√¢u h·ªèi/t·ª´ kh√≥a t√¨m ki·∫øm
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ (m·∫∑c ƒë·ªãnh t·ª´ config)
            threshold: Ng∆∞·ª°ng similarity (m·∫∑c ƒë·ªãnh t·ª´ config)
            
        Returns:
            Dict ch·ª©a results v√† metadata
        """
        top_k = top_k or RAG_MAX_RESULTS
        threshold = threshold or RAG_SIMILARITY_THRESHOLD
        
        logger.info(f"üîç Search query: '{query}' (top_k={top_k}, threshold={threshold})")
        
        try:
            # Encode query
            query_embedding = self.embedding_model.encode([query])[0].tolist()
            
            # Search trong Milvus (s·ª≠ d·ª•ng L2 distance thay v√¨ COSINE)
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
            results = self.collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                expr=None,
                output_fields=["text", "page", "pdf_source"]
            )
            
            # Parse results
            if not results or not results[0]:
                logger.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p")
                return {
                    "success": False,
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan",
                    "results": [],
                    "count": 0
                }
            
            # Filter by threshold v√† format results
            # V·ªõi L2 distance: c√†ng nh·ªè c√†ng gi·ªëng nhau
            # L2 distance range: [0, ‚àû), n√™n c√†ng nh·ªè c√†ng t·ªët
            # ƒê·ªÉ d·ªÖ hi·ªÉu: chuy·ªÉn L2 sang similarity score [0, 1]
            # similarity = 1 / (1 + distance)
            formatted_results = []
            
            logger.info(f"üìä Distances from Milvus:")
            for hit in results[0]:
                # Chuy·ªÉn L2 distance th√†nh similarity score
                similarity = 1.0 / (1.0 + hit.distance)
                logger.info(f"   - Distance: {hit.distance:.4f}, Similarity: {similarity:.4f}")
                
                # Ch·ªâ l·∫•y k·∫øt qu·∫£ c√≥ similarity >= threshold
                if similarity >= threshold:
                    formatted_results.append({
                        "text": hit.entity.get('text'),
                        "page": hit.entity.get('page'),
                        "source": hit.entity.get('pdf_source'),
                        "score": float(similarity)
                    })
            
            logger.info(f"‚úÖ T√¨m th·∫•y {len(formatted_results)} k·∫øt qu·∫£")
            
            return {
                "success": True,
                "results": formatted_results,
                "count": len(formatted_results),
                "query": query
            }
            
        except Exception as e:
            logger.error(f"L·ªói khi search: {e}")
            return {
                "success": False,
                "message": f"L·ªói: {str(e)}",
                "results": [],
                "count": 0
            }
    
    def ask(
        self, 
        question: str,
        top_k: Optional[int] = None,
        return_context: bool = False
    ) -> Dict[str, Any]:
        """
        H·ªèi c√¢u h·ªèi v√† nh·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ RAG system.
        
        Args:
            question: C√¢u h·ªèi
            top_k: S·ªë context documents (m·∫∑c ƒë·ªãnh t·ª´ config)
            return_context: C√≥ tr·∫£ v·ªÅ context kh√¥ng
            
        Returns:
            Dict ch·ª©a answer, sources v√† metadata
        """
        top_k = top_k or RAG_MAX_RESULTS
        
        logger.info(f"‚ùì Question: '{question}'")
        
        try:
            # Search context
            search_result = self.search(question, top_k=top_k)
            
            if not search_result["success"] or search_result["count"] == 0:
                return {
                    "success": False,
                    "answer": "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "sources": [],
                    "context": None
                }
            
            # Build context t·ª´ search results
            context_parts = []
            sources = []
            
            for i, result in enumerate(search_result["results"], 1):
                context_parts.append(f"[ƒêo·∫°n {i} - Trang {result['page']}]:\n{result['text']}")
                sources.append({
                    "page": result["page"],
                    "source": result["source"],
                    "score": result["score"]
                })
            
            context = "\n\n".join(context_parts)
            
            # Build prompt
            prompt = f"""D·ª±a tr√™n c√°c th√¥ng tin sau ƒë√¢y t·ª´ t√†i li·ªáu:

{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}

L∆∞u √Ω:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p
- Tr√≠ch d·∫´n s·ªë trang khi c√≥ th·ªÉ
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch
"""
            
            # Generate answer
            logger.info("ƒêang generate c√¢u tr·∫£ l·ªùi...")
            answer = generate_answer_with_fallback(
                prompt,
                self.model_choice,
                self.llm_client,
                self.ollama_model_name
            )
            
            logger.info("‚úÖ ƒê√£ generate c√¢u tr·∫£ l·ªùi")
            
            result = {
                "success": True,
                "answer": answer,
                "sources": sources,
                "query": question
            }
            
            if return_context:
                result["context"] = context
            
            return result
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
            return {
                "success": False,
                "answer": f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}",
                "sources": [],
                "context": None
            }
    
    def get_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ tool."""
        return {
            "name": self.name,
            "description": self.description,
            "collection": COLLECTION_NAME,
            "embedding_model": EMBEDDING_MODEL_NAME,
            "max_results": RAG_MAX_RESULTS,
            "threshold": RAG_SIMILARITY_THRESHOLD
        }


# --- TOOL INTERFACE CHO AGENT ---

def create_rag_tool() -> RAGTool:
    """
    Factory function ƒë·ªÉ t·∫°o RAG tool.
    
    Returns:
        RAGTool instance
    """
    return RAGTool()


# --- TEST & DEMO ---
if __name__ == "__main__":
    print("=== Testing RAG Tool ===\n")
    
    # Kh·ªüi t·∫°o tool
    print("1. Kh·ªüi t·∫°o RAG Tool...")
    tool = create_rag_tool()
    print(f"   ‚úÖ Tool info: {tool.get_info()}\n")
    
    # Test search
    print("2. Test search...")
    search_result = tool.search("ch·ªâ s·ªë ROUGE", top_k=15)  # Vietnamese query, top_k=15
    print(f"   Found: {search_result['count']} results")
    if search_result['results']:
        print(f"   Top result: {search_result['results'][0]['text'][:100]}...")
        print(f"   Score: {search_result['results'][0]['score']:.4f}\n")
    
    # Test ask
    print("\n3. Test ask...")
    answer_result = tool.ask("ROUGE l√† g√¨?", top_k=15)  # Vietnamese question, top_k=15
    print(f"   Answer: {answer_result['answer'][:200]}...")
    print(f"   Sources: {len(answer_result['sources'])} documents\n")
    
    print("=== Test completed ===")
