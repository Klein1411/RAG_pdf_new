import sys
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
import os
import re

# Th√™m th∆∞ m·ª•c g·ªëc project v√†o sys.path ƒë·ªÉ import src module
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt t·ª´ c√°c file kh√°c
from src.milvus import get_or_create_collection
from src.config import PDF_PATH, EMBEDDING_MODEL_NAME, EMBEDDING_DIM, COLLECTION_NAME, OUTPUT_DIR
from src.export_md import convert_to_markdown
from src.logging_config import get_logger

import nltk

logger = get_logger(__name__)

def get_embedding_model():
    """
    Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ model embedding, ∆∞u ti√™n s·ª≠ d·ª•ng GPU n·∫øu c√≥.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"T·∫£i model embedding '{EMBEDDING_MODEL_NAME}' l√™n device '{device}'")
    print(f"ü§ñ ƒêang t·∫£i model embedding '{EMBEDDING_MODEL_NAME}' l√™n '{device}'...")
    
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
        logger.info("Model embedding ƒë√£ t·∫£i th√†nh c√¥ng")
        print("   -> ‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!")
        return model
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i model embedding: {e}")
        print(f"   -> ‚ùå L·ªói khi t·∫£i model: {e}")
        return None

def download_nltk_punkt():
    """
    Ki·ªÉm tra v√† t·∫£i v·ªÅ c√°c g√≥i tokenizer c·∫ßn thi·∫øt c·ªßa NLTK.
    """
    try:
        # Ph·∫£i ki·ªÉm tra c·∫£ hai t√†i nguy√™n, n·∫øu m·ªôt trong hai thi·∫øu, s·∫Ω g√¢y ra LookupError
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('tokenizers/punkt_tab')
        logger.info("NLTK tokenizer ƒë√£ c√≥ s·∫µn")
    except LookupError:
        logger.info("T·∫£i c√°c g√≥i tokenizer NLTK (punkt, punkt_tab)")
        print("   -> üìñ ƒêang t·∫£i c√°c g√≥i tokenizer c·∫ßn thi·∫øt cho NLTK ('punkt', 'punkt_tab')...")
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)
        logger.info("T·∫£i tokenizer ho√†n t·∫•t")
        print("   -> ‚úÖ T·∫£i tokenizer ho√†n t·∫•t.")

def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap_sentences: int = 2):
    """
    Chia vƒÉn b·∫£n th√†nh c√°c chunk m·ªôt c√°ch th√¥ng minh, d·ª±a tr√™n ranh gi·ªõi c√¢u.
    S·ª≠ d·ª•ng NLTK ƒë·ªÉ t√°ch c√¢u v√† nh√≥m ch√∫ng l·∫°i, v·ªõi s·ª± g·ªëi ƒë·∫ßu gi·ªØa c√°c chunk.
    
    Args:
        text (str): ƒêo·∫°n vƒÉn b·∫£n c·∫ßn chia.
        chunk_size (int): K√≠ch th∆∞·ªõc t·ªëi ƒëa (s·ªë k√Ω t·ª±) c·ªßa m·ªói chunk.
        chunk_overlap_sentences (int): S·ªë c√¢u g·ªëi ƒë·∫ßu gi·ªØa c√°c chunk li√™n ti·∫øp.
    """
    if not text:
        return []

    # 1. T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u
    sentences = nltk.sent_tokenize(text)
    
    # 2. Nh√≥m c√°c c√¢u th√†nh c√°c chunk
    chunks = []
    current_chunk_sentences = []
    current_length = 0
    
    for i, sentence in enumerate(sentences):
        sentence_length = len(sentence)
        
        # N·∫øu th√™m c√¢u n√†y v√†o s·∫Ω qu√° d√†i, h√£y ho√†n th√†nh chunk hi·ªán t·∫°i
        if current_length + sentence_length > chunk_size and current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))
            
            # B·∫Øt ƒë·∫ßu chunk m·ªõi v·ªõi s·ª± g·ªëi ƒë·∫ßu
            # L·∫•y N c√¢u cu·ªëi t·ª´ chunk v·ª´a ho√†n th√†nh ƒë·ªÉ l√†m ph·∫ßn g·ªëi ƒë·∫ßu
            overlap_start_index = max(0, len(current_chunk_sentences) - chunk_overlap_sentences)
            current_chunk_sentences = current_chunk_sentences[overlap_start_index:]
            # C·∫ßn t√≠nh l·∫°i ƒë·ªô d√†i c·ªßa chunk m·ªõi sau khi c√≥ overlap
            current_length = len(" ".join(current_chunk_sentences))

        # Th√™m c√¢u v√†o chunk hi·ªán t·∫°i (d√π l√† chunk m·ªõi hay c≈©)
        current_chunk_sentences.append(sentence)
        current_length += sentence_length

    # ƒê·ª´ng qu√™n chunk cu·ªëi c√πng
    if current_chunk_sentences:
        chunks.append(" ".join(current_chunk_sentences))
        
    return chunks

def populate_database():
    """
    H√†m ch√≠nh: T·ª± ƒë·ªông t·∫°o file Markdown n·∫øu c·∫ßn, ƒë·ªçc n·ªôi dung,
    t·∫°o embedding, v√† l∆∞u v√†o Milvus.
    """
    download_nltk_punkt() # ƒê·∫£m b·∫£o NLTK ƒë√£ s·∫µn s√†ng
    logger.info("=== B·∫ÆT ƒê·∫¶U QU√Å TR√åNH ƒê·ªíNG B·ªò D·ªÆ LI·ªÜU V√ÄO MILVUS ===")
    print("--- B·∫ÆT ƒê·∫¶U QU√Å TR√åNH ƒê·ªíNG B·ªò D·ªÆ LI·ªÜU V√ÄO MILVUS ---")

    # --- B∆∞·ªõc 1: ƒê·∫£m b·∫£o file Markdown t·ªìn t·∫°i ---
    logger.info("B∆∞·ªõc 1: Chu·∫©n b·ªã file Markdown ngu·ªìn")
    print("\n--- B∆∞·ªõc 1: Chu·∫©n b·ªã file Markdown ngu·ªìn ---")
    md_filename = os.path.splitext(os.path.basename(PDF_PATH))[0] + ".md"
    md_filepath = os.path.join(OUTPUT_DIR, md_filename)

    if not os.path.exists(md_filepath):
        logger.warning(f"File '{md_filename}' kh√¥ng t·ªìn t·∫°i trong {OUTPUT_DIR}, t·∫°o m·ªõi t·ª´ PDF")
        print(f"   -> ‚ö†Ô∏è File '{md_filename}' kh√¥ng t·ªìn t·∫°i trong {OUTPUT_DIR}. T·ª± ƒë·ªông t·∫°o m·ªõi t·ª´ PDF...")
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        markdown_content = convert_to_markdown(PDF_PATH)
        try:
            with open(md_filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            logger.info(f"ƒê√£ t·∫°o v√† l∆∞u file '{md_filename}' v√†o {OUTPUT_DIR}")
            print(f"   -> ‚úÖ ƒê√£ t·∫°o v√† l∆∞u file '{md_filename}' v√†o {OUTPUT_DIR}.")
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u file Markdown: {e}")
            print(f"   -> ‚ùå L·ªói khi l∆∞u file Markdown: {e}")
            return
    else:
        logger.info(f"ƒê√£ t√¨m th·∫•y file '{md_filename}' trong {OUTPUT_DIR}")
        print(f"   -> ‚úÖ ƒê√£ t√¨m th·∫•y file '{md_filename}' trong {OUTPUT_DIR}.")

    # --- B∆∞·ªõc 2: ƒê·ªçc v√† x·ª≠ l√Ω file Markdown ---
    logger.info(f"B∆∞·ªõc 2: ƒê·ªçc file {md_filename}")
    print(f"\n--- B∆∞·ªõc 2: ƒê·ªçc v√† x·ª≠ l√Ω file: {md_filename} ---")
    try:
        with open(md_filepath, "r", encoding="utf-8") as f:
            full_content = f.read()
        logger.info(f"ƒê·ªçc file th√†nh c√¥ng ({len(full_content)} k√Ω t·ª±)")
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file Markdown: {e}")
        print(f"   -> ‚ùå L·ªói khi ƒë·ªçc file Markdown: {e}")
        return

    # --- B∆∞·ªõc 3: Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt ---
    model = get_embedding_model()
    if not model:
        logger.error("Kh√¥ng th·ªÉ t·∫£i model embedding")
        return

    logger.info(f"B∆∞·ªõc 3: Chu·∫©n b·ªã collection '{COLLECTION_NAME}' tr√™n Milvus")
    print("\n--- B∆∞·ªõc 3: Chu·∫©n b·ªã collection tr√™n Milvus ---")
    collection = get_or_create_collection(COLLECTION_NAME, dim=EMBEDDING_DIM, recreate=True)

    # --- B∆∞·ªõc 4: Ph√¢n t√°ch n·ªôi dung v√† t·∫°o chunks ---
    logger.info("B∆∞·ªõc 4: Ph√¢n t√°ch n·ªôi dung v√† t·∫°o chunks")
    print("\n--- B∆∞·ªõc 4: Ph√¢n t√°ch n·ªôi dung v√† t·∫°o chunks ---")
    all_chunks = []
    metadata = []

    # T√°ch file MD th√†nh c√°c trang d·ª±a tr√™n marker
    # Regex ƒë·ªÉ t√¨m "--- Trang X (Ngu·ªìn: Y) ---"
    page_splits = re.split(r'--- Trang (\d+) \(Ngu·ªìn: [^)]+\) ---', full_content)

    # B·ªè ph·∫ßn ƒë·∫ßu ti√™n (th∆∞·ªùng l√† r·ªóng ho·∫∑c l√† ti√™u ƒë·ªÅ ch√≠nh)
    content_parts = page_splits[1:]
    
    if not content_parts:
        logger.warning("Kh√¥ng t√¨m th·∫•y marker trang trong MD, coi to√†n b·ªô file l√† 1 trang")
        print("   -> ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y marker trang n√†o trong file MD. Coi to√†n b·ªô file l√† m·ªôt trang.")
        page_chunks = chunk_text(full_content)
        all_chunks.extend(page_chunks)
        for _ in page_chunks:
            metadata.append({"pdf_source": os.path.basename(PDF_PATH), "page": 1})
    else:
        # Gh√©p c·∫∑p [s·ªë trang, n·ªôi dung]
        for i in range(0, len(content_parts), 2):
            page_num = int(content_parts[i])
            page_content = content_parts[i+1].strip()
            
            if not page_content:
                continue

            page_chunks = chunk_text(page_content)
            all_chunks.extend(page_chunks)
            
            # G√°n c√πng m·ªôt s·ªë trang cho t·∫•t c·∫£ c√°c chunk c·ªßa trang ƒë√≥
            for _ in page_chunks:
                metadata.append({
                    "pdf_source": os.path.basename(PDF_PATH),
                    "page": page_num
                })
        logger.info(f"X·ª≠ l√Ω ƒë∆∞·ª£c {len(set(m['page'] for m in metadata))} trang")
        print(f"   -> ƒê√£ x·ª≠ l√Ω v√† ph√¢n t√°ch ƒë∆∞·ª£c {len(set(m['page'] for m in metadata))} trang.")

    if not all_chunks:
        logger.error("Kh√¥ng t√¨m th·∫•y ƒëo·∫°n vƒÉn b·∫£n n√†o ƒë·ªÉ x·ª≠ l√Ω")
        print("‚ùå Kh√¥ng t√¨m th·∫•y ƒëo·∫°n vƒÉn b·∫£n n√†o ƒë·ªÉ x·ª≠ l√Ω.")
        return
    
    logger.info(f"T·ªïng c·ªông c√≥ {len(all_chunks)} chunks c·∫ßn x·ª≠ l√Ω")
    print(f"   -> T·ªïng c·ªông c√≥ {len(all_chunks)} ƒëo·∫°n vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω.")

    # --- B∆∞·ªõc 5: T·∫°o embedding cho t·∫•t c·∫£ c√°c chunks ---
    logger.info("B∆∞·ªõc 5: T·∫°o embeddings cho c√°c ƒëo·∫°n vƒÉn b·∫£n")
    print("\n--- B∆∞·ªõc 5: T·∫°o embeddings cho c√°c ƒëo·∫°n vƒÉn b·∫£n ---")
    embeddings = model.encode(all_chunks, show_progress_bar=True)
    logger.info("T·∫°o embedding ho√†n t·∫•t")
    print("   -> ‚úÖ T·∫°o embedding ho√†n t·∫•t.")

    # --- B∆∞·ªõc 6: Chu·∫©n b·ªã v√† l∆∞u d·ªØ li·ªáu v√†o Milvus ---
    logger.info("B∆∞·ªõc 6: L∆∞u d·ªØ li·ªáu v√†o Milvus")
    print("\n--- B∆∞·ªõc 6: L∆∞u d·ªØ li·ªáu v√†o Milvus ---")
    entities = [
        embeddings,
        all_chunks,
        [meta['pdf_source'] for meta in metadata],
        [meta['page'] for meta in metadata]
    ]
    
    try:
        insert_result = collection.insert(entities)
        logger.info(f"Ch√®n th√†nh c√¥ng {insert_result.insert_count} vectors v√†o Milvus")
        print(f"   -> ‚úÖ Ch√®n th√†nh c√¥ng {insert_result.insert_count} vectors v√†o Milvus.")
        
        logger.info("ƒêang flush collection")
        print("   -> ƒêang flush collection...")
        collection.flush()
        logger.info("Flush ho√†n t·∫•t")
        print("   -> ‚úÖ Flush ho√†n t·∫•t.")

    except Exception as e:
        logger.error(f"L·ªói khi ch√®n d·ªØ li·ªáu v√†o Milvus: {e}")
        print(f"   -> ‚ùå L·ªói khi ch√®n d·ªØ li·ªáu v√†o Milvus: {e}")

    logger.info("=== QU√Å TR√åNH ƒê·ªíNG B·ªò HO√ÄN T·∫§T ===")
    print("\n--- QU√Å TR√åNH ƒê·ªíNG B·ªò HO√ÄN T·∫§T ---")


if __name__ == "__main__":
    populate_database()
