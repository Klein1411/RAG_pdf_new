"""
T√≠ch h·ª£p LangChain LLM - Giao di·ªán LLM h·ª£p nh·∫•t s·ª≠ d·ª•ng LangChain

Hybrid approach:
- Gemini: google-generativeai tr·ª±c ti·∫øp (v√¨ LangChain v1beta API kh√¥ng support nhi·ªÅu models m·ªõi)
- Ollama: LangChain wrapper (ho·∫°t ƒë·ªông t·ªët)

L·ª£i √≠ch:
- Gemini: Truy c·∫≠p ƒë·∫ßy ƒë·ªß 41+ models m·ªõi nh·∫•t (gemini-2.5-flash, gemini-2.0-flash, v.v.)
- Ollama: D√πng LangChain cho t√≠ch h·ª£p t·ªët v√† streaming
- Interface th·ªëng nh·∫•t cho c·∫£ hai
- Auto-fallback qua multiple keys v√† models
"""

import os
import sys
import time
from pathlib import Path
from typing import Optional, List, Dict, Any, Union
from dotenv import load_dotenv

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path ƒë·ªÉ import ƒë∆∞·ª£c src module
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Nh·∫≠p LangChain (cho Ollama)
from langchain_core.language_models import BaseLanguageModel
from langchain_community.llms import Ollama

# Nh·∫≠p google-generativeai tr·ª±c ti·∫øp (cho Gemini)
import google.generativeai as genai

from src.logging_config import get_logger

logger = get_logger(__name__)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# Nh·∫≠p c√°c model Gemini t·ª´ config
try:
    from src.config import GEMINI_MODELS
except ImportError:
    # Fallback n·∫øu kh√¥ng c√≥ trong config  
    GEMINI_MODELS = [
        "models/gemini-2.5-flash",     # M·ªõi nh·∫•t
        "models/gemini-2.0-flash",     # D·ª± ph√≤ng
        "models/gemini-flash-latest",  # Stable alias
        "models/gemini-pro-latest"     # Fallback
    ]
    logger.warning("Kh√¥ng t√¨m th·∫•y GEMINI_MODELS trong config, d√πng danh s√°ch m·∫∑c ƒë·ªãnh")


class LLMManager:
    """
    Tr√¨nh qu·∫£n l√Ω LLM h·ª£p nh·∫•t.
    
    Hybrid approach:
    - Google Gemini: D√πng google-generativeai tr·ª±c ti·∫øp (genai.GenerativeModel)
    - Ollama: D√πng LangChain wrapper (langchain-community.llms.Ollama)
    
    T√≠nh nƒÉng:
    - T·ª± ƒë·ªông ch·ªçn nh√† cung c·∫•p
    - T·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi API key khi g·∫∑p l·ªói (Gemini)
    - T·ª± ƒë·ªông th·ª≠ c√°c model fallback (Gemini)
    - Logic th·ª≠ l·∫°i v·ªõi auto-recovery
    - X·ª≠ l√Ω l·ªói th·ªëng nh·∫•t
    - Interface generate() chung cho c·∫£ 2 providers
    """
    
    def __init__(
        self,
        provider: str = "gemini",  # "gemini" ho·∫∑c "ollama"
        model_name: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ):
        """
        Kh·ªüi t·∫°o Tr√¨nh qu·∫£n l√Ω LLM.
        
        ƒê·ªëi s·ªë:
            provider: Nh√† cung c·∫•p LLM ("gemini" ho·∫∑c "ollama")
            model_name: T√™n m√¥ h√¨nh (n·∫øu None, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh)
            temperature: Nhi·ªát ƒë·ªô l·∫•y m·∫´u (0-1)
            max_tokens: S·ªë l∆∞·ª£ng token t·ªëi ƒëa ƒë·ªÉ t·∫°o
        """
        self.provider = provider
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        # Type: Union v√¨ Gemini d√πng GenerativeModel, Ollama d√πng LangChain
        self.llm: Optional[Union[Any, BaseLanguageModel]] = None  # type: ignore
        
        # Qu·∫£n l√Ω API keys v√† models cho Gemini
        self.gemini_api_keys: List[str] = []
        self.current_key_index: int = 0
        self.available_models: List[str] = GEMINI_MODELS.copy()
        self.current_model_index: int = 0
        
        # Initialize LLM
        self._initialize_llm()
    
    def _initialize_llm(self):
        """Kh·ªüi t·∫°o LangChain LLM d·ª±a tr√™n nh√† cung c·∫•p."""
        if self.provider == "gemini":
            self._initialize_gemini()
        elif self.provider == "ollama":
            self._initialize_ollama()
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def _initialize_gemini(self):
        """Kh·ªüi t·∫°o Google Gemini qua LangChain v·ªõi auto-retry."""
        # L·∫•y t·∫•t c·∫£ API keys
        self.gemini_api_keys = self._get_gemini_api_keys()
        
        if not self.gemini_api_keys:
            raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y Gemini API key trong .env")
        
        logger.info(f"üìã T√¨m th·∫•y {len(self.gemini_api_keys)} Gemini API key(s)")
        logger.info(f"üìã C√≥ {len(self.available_models)} model(s): {', '.join(self.available_models)}")
        
        # Th·ª≠ t·∫•t c·∫£ c√°c k·∫øt h·ª£p key + model cho ƒë·∫øn khi th√†nh c√¥ng
        for key_idx in range(len(self.gemini_api_keys)):
            self.current_key_index = key_idx
            for model_idx in range(len(self.available_models)):
                self.current_model_index = model_idx
                
                if self._try_initialize_gemini():
                    return  # Th√†nh c√¥ng!
        
        # ƒê√£ th·ª≠ h·∫øt t·∫•t c·∫£ k·∫øt h·ª£p
        raise RuntimeError("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini v·ªõi b·∫•t k·ª≥ API key ho·∫∑c model n√†o")
    
    def _try_initialize_gemini(self) -> bool:
        """
        Th·ª≠ kh·ªüi t·∫°o Gemini v·ªõi key v√† model hi·ªán t·∫°i.
        D√πng google-generativeai tr·ª±c ti·∫øp.
        Returns True n·∫øu th√†nh c√¥ng.
        """
        try:
            api_key = self.gemini_api_keys[self.current_key_index]
            
            # Lu√¥n d√πng model t·ª´ available_models theo index (ƒë·ªÉ loop qua t·∫•t c·∫£)
            model = self.available_models[self.current_model_index]
            
            # Mask API key cho log
            masked_key = api_key[:8] + "..." + api_key[-4:] if len(api_key) > 12 else "***"
            logger.info(f"üîë ƒêang th·ª≠ API key [{self.current_key_index + 1}/{len(self.gemini_api_keys)}]: {masked_key}")
            logger.info(f"ü§ñ ƒêang th·ª≠ model [{self.current_model_index + 1}/{len(self.available_models)}]: {model}")
            
            # Configure google-generativeai
            genai.configure(api_key=api_key)
            
            # Kh·ªüi t·∫°o model
            generation_config = {
                "temperature": self.temperature,
            }
            if self.max_tokens:
                generation_config["max_output_tokens"] = self.max_tokens
            
            self.llm = genai.GenerativeModel(
                model_name=model,
                generation_config=generation_config
            )
            
            # Test v·ªõi prompt ƒë∆°n gi·∫£n
            test_response = self.llm.generate_content("Hi")
            
            logger.info(f"‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng Gemini: {model} (Key {self.current_key_index + 1})")
            logger.info(f"   Test response: {test_response.text[:50]}...")
            self.model_name = model  # C·∫≠p nh·∫≠t model_name
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Th·∫•t b·∫°i v·ªõi key {self.current_key_index + 1}, model {model}: {str(e)[:100]}")
            return False
    
    def _initialize_ollama(self):
        """Initialize Ollama via LangChain."""
        try:
            # Default model if not specified
            model = self.model_name or "llama3:latest"
            
            # Initialize LangChain Ollama
            self.llm = Ollama(
                model=model,
                temperature=self.temperature,
                num_predict=self.max_tokens
            )
            
            logger.info(f"‚úÖ Initialized Ollama LLM: {model}")
            
        except Exception as e:
            logger.error(f"Failed to initialize Ollama: {e}")
            raise
    
    def _get_gemini_api_keys(self) -> List[str]:
        """
        L·∫•y t·∫•t c·∫£ Gemini API keys t·ª´ m√¥i tr∆∞·ªùng.
        H·ªó tr·ª£: 
        - GEMINI_API_KEY ho·∫∑c GEMINI_API_KEY_1
        - GEMINI_API_KEY_2
        - GEMINI_API_KEY_3
        - GEMINI_API_KEY_4
        """
        keys = []
        for i in range(1, 5):  # H·ªó tr·ª£ t·ªõi 4 keys
            # Th·ª≠ c·∫£ 2 format: GEMINI_API_KEY v√† GEMINI_API_KEY_1
            if i == 1:
                key_names = ["GEMINI_API_KEY", "GEMINI_API_KEY_1"]
            else:
                key_names = [f"GEMINI_API_KEY_{i}"]
            
            for key_name in key_names:
                key = os.getenv(key_name)
                if key and key.strip():
                    keys.append(key.strip())
                    logger.debug(f"‚úì T√¨m th·∫•y {key_name}")
                    break  # ƒê√£ t√¨m th·∫•y key cho slot n√†y, kh√¥ng c·∫ßn th·ª≠ format kh√°c
        
        return keys
    
    def switch_gemini_key(self) -> bool:
        """
        Chuy·ªÉn sang API key ti·∫øp theo.
        Returns True n·∫øu c√≤n key ƒë·ªÉ th·ª≠.
        """
        if self.provider != "gemini":
            return False
        
        self.current_key_index += 1
        
        if self.current_key_index >= len(self.gemini_api_keys):
            logger.warning("‚ö†Ô∏è ƒê√£ h·∫øt API key ƒë·ªÉ th·ª≠")
            self.current_key_index = 0  # Reset v·ªÅ key ƒë·∫ßu
            return False
        
        logger.info(f"üîÑ Chuy·ªÉn sang API key {self.current_key_index + 1}/{len(self.gemini_api_keys)}")
        
        # Th·ª≠ kh·ªüi t·∫°o l·∫°i v·ªõi key m·ªõi
        return self._try_initialize_gemini()
    
    def switch_gemini_model(self) -> bool:
        """
        Chuy·ªÉn sang model ti·∫øp theo trong danh s√°ch GEMINI_MODELS.
        Returns True n·∫øu c√≤n model ƒë·ªÉ th·ª≠.
        """
        if self.provider != "gemini":
            return False
        
        self.current_model_index += 1
        
        if self.current_model_index >= len(self.available_models):
            logger.warning("‚ö†Ô∏è ƒê√£ th·ª≠ h·∫øt t·∫•t c·∫£ models")
            self.current_model_index = 0  # Reset v·ªÅ model ƒë·∫ßu
            return False
        
        logger.info(f"üîÑ Chuy·ªÉn sang model {self.current_model_index + 1}/{len(self.available_models)}")
        
        # Th·ª≠ kh·ªüi t·∫°o l·∫°i v·ªõi model m·ªõi
        return self._try_initialize_gemini()
    
    def auto_recover(self) -> bool:
        """
        T·ª± ƒë·ªông kh√¥i ph·ª•c khi g·∫∑p l·ªói b·∫±ng c√°ch th·ª≠:
        1. Model ti·∫øp theo v·ªõi key hi·ªán t·∫°i
        2. Key ti·∫øp theo v·ªõi model ƒë·∫ßu ti√™n
        
        Returns True n·∫øu kh√¥i ph·ª•c th√†nh c√¥ng.
        """
        if self.provider != "gemini":
            return False
        
        logger.info("üîß ƒêang t·ª± ƒë·ªông kh√¥i ph·ª•c...")
        
        # Th·ª≠ 1: Chuy·ªÉn model
        if self.switch_gemini_model():
            return True
        
        # Th·ª≠ 2: Chuy·ªÉn key (reset model v·ªÅ ƒë·∫ßu)
        self.current_model_index = 0
        if self.switch_gemini_key():
            return True
        
        logger.error("‚ùå Kh√¥ng th·ªÉ t·ª± ƒë·ªông kh√¥i ph·ª•c")
        return False
    
    def generate(self, prompt: str, auto_retry: bool = True, **kwargs) -> str:
        """
        Generate text from prompt v·ªõi t·ª± ƒë·ªông retry.
        
        Hybrid approach:
        - Gemini: D√πng google-generativeai.GenerativeModel.generate_content()
        - Ollama: D√πng LangChain invoke()
        
        Args:
            prompt: Input prompt
            auto_retry: T·ª± ƒë·ªông th·ª≠ l·∫°i v·ªõi key/model kh√°c n·∫øu l·ªói
            **kwargs: Additional arguments for LLM
            
        Returns:
            Generated text
        """
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                if self.llm is None:
                    raise RuntimeError("LLM ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
                
                # Gemini: D√πng generate_content() tr·ª±c ti·∫øp
                if self.provider == "gemini":
                    response = self.llm.generate_content(prompt, **kwargs)  # type: ignore
                    return response.text  # type: ignore
                
                # Ollama: D√πng LangChain invoke()
                else:
                    response = self.llm.invoke(prompt, **kwargs)
                    # Extract text from response
                    if hasattr(response, 'content'):
                        return str(response.content)
                    else:
                        return str(response)
                    
            except Exception as e:
                error_msg = str(e).lower()
                retry_count += 1
                
                logger.error(f"‚ùå L·ªói generation (l·∫ßn {retry_count}/{max_retries}): {e}")
                
                # Ki·ªÉm tra n·∫øu l√† l·ªói API key ho·∫∑c quota
                is_api_error = any(keyword in error_msg for keyword in [
                    'api key', 'quota', 'rate limit', 'invalid', 'expired',
                    'permission', 'forbidden', '429', '401', '403', '404'
                ])
                
                if auto_retry and is_api_error and self.provider == "gemini":
                    logger.info("üîÑ ƒêang th·ª≠ kh√¥i ph·ª•c t·ª± ƒë·ªông...")
                    
                    if self.auto_recover():
                        logger.info("‚úÖ Kh√¥i ph·ª•c th√†nh c√¥ng, th·ª≠ l·∫°i...")
                        continue
                    else:
                        logger.error("‚ùå Kh√¥ng th·ªÉ kh√¥i ph·ª•c")
                        raise
                else:
                    # L·ªói kh√°c ho·∫∑c h·∫øt l∆∞·ª£t retry
                    if retry_count >= max_retries:
                        logger.error(f"‚ùå ƒê√£ th·ª≠ {max_retries} l·∫ßn, v·∫´n th·∫•t b·∫°i")
                    raise
        
        # Fallback n·∫øu v∆∞·ª£t qu√° max_retries (kh√¥ng n√™n ƒë·∫øn ƒë√¢y)
        raise RuntimeError("ƒê√£ v∆∞·ª£t qu√° s·ªë l·∫ßn th·ª≠ t·ªëi ƒëa")
    
    def generate_with_history(
        self,
        prompt: str,
        history: Optional[List[Dict[str, str]]] = None,
        **kwargs
    ) -> str:
        """
        Generate with conversation history.
        
        NOTE: Hi·ªán t·∫°i ch·ªâ support cho Ollama (d√πng LangChain messages).
        Gemini s·∫Ω d√πng format kh√°c (ch∆∞a implement).
        
        Args:
            prompt: Current prompt
            history: List of {'role': 'user'/'assistant', 'content': str}
            **kwargs: Additional arguments
            
        Returns:
            Generated text
        """
        if self.provider == "gemini":
            # Gemini d√πng google-generativeai chat format (ch∆∞a implement)
            # T·∫°m th·ªùi fallback v·ªÅ generate() th√¥ng th∆∞·ªùng
            logger.warning("generate_with_history ch∆∞a support Gemini, d√πng generate() thay th·∫ø")
            return self.generate(prompt, **kwargs)
        
        # Ollama: D√πng LangChain messages (c·∫ßn import)
        try:
            from langchain_core.messages import HumanMessage, SystemMessage
            
            if self.llm is None:
                raise RuntimeError("LLM not initialized")
            
            # Build messages from history
            messages = []
            
            if history:
                for msg in history:
                    if msg['role'] == 'user':
                        messages.append(HumanMessage(content=msg['content']))
                    elif msg['role'] == 'assistant':
                        messages.append(SystemMessage(content=msg['content']))
            
            # Add current prompt
            messages.append(HumanMessage(content=prompt))
            
            # Generate
            response = self.llm.invoke(messages, **kwargs)
            
            # Extract text
            if hasattr(response, 'content'):
                return str(response.content)  # type: ignore
            else:
                return str(response)
                
        except Exception as e:
            logger.error(f"Generation with history error: {e}")
            raise
    
    def switch_provider(self, provider: str, model_name: Optional[str] = None):
        """
        Switch to different provider.
        
        Args:
            provider: New provider name
            model_name: Optional model name
        """
        self.provider = provider
        if model_name:
            self.model_name = model_name
        self._initialize_llm()
    
    def get_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin LLM hi·ªán t·∫°i."""
        info = {
            'provider': self.provider,
            'model': self.model_name,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens
        }
        
        if self.provider == "gemini":
            info.update({
                'current_key_index': self.current_key_index + 1,
                'total_keys': len(self.gemini_api_keys),
                'current_model_index': self.current_model_index + 1,
                'available_models': self.available_models
            })
        
        return info
    
    def get_langchain_llm(self) -> BaseLanguageModel:
        """
        Tr·∫£ v·ªÅ LangChain-compatible LLM cho RAG chains.
        
        NOTE: Gemini hi·ªán d√πng google-generativeai tr·ª±c ti·∫øp (kh√¥ng ph·∫£i LangChain Runnable).
        Method n√†y ch·ªâ support Ollama cho LangChain chains.
        
        N·∫øu provider l√† Gemini, s·∫Ω raise error v√¨ kh√¥ng t∆∞∆°ng th√≠ch v·ªõi LangChain chains.
        Khuy·∫øn ngh·ªã: D√πng generate() method tr·ª±c ti·∫øp thay v√¨ LangChain chains.
        
        Returns:
            BaseLanguageModel cho LangChain chains (ch·ªâ Ollama)
            
        Raises:
            ValueError n·∫øu provider l√† Gemini
        """
        if self.provider == "gemini":
            raise ValueError(
                "Gemini kh√¥ng support LangChain chains (d√πng google-generativeai tr·ª±c ti·∫øp). "
                "Khuy·∫øn ngh·ªã: D√πng llm_manager.generate() thay v√¨ chains, "
                "ho·∫∑c switch sang Ollama v·ªõi llm_manager.switch_provider('ollama')"
            )
        
        if self.llm is None:
            raise RuntimeError("LLM ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        return self.llm  # type: ignore


# Convenience functions (backward compatibility)

def initialize_and_select_llm_langchain() -> tuple[str, LLMManager]:
    """
    Interactive LLM selection (LangChain version).
    
    Returns:
        (model_choice, llm_manager)
        model_choice: "1" for Gemini, "2" for Ollama
    """
    print("\nü§ñ Ch·ªçn LLM Provider:")
    print("1. Google Gemini (Cloud API - Auto fallback)")
    print("2. Ollama (Local)")
    
    choice = input("Ch·ªçn (1/2): ").strip()
    
    if choice == "1":
        # Gemini - hi·ªÉn th·ªã models t·ª´ config
        print(f"\nüìã Available Gemini models ({len(GEMINI_MODELS)}):")
        for i, model in enumerate(GEMINI_MODELS, 1):
            print(f"{i}. {model}")
        
        model_idx = input(f"Ch·ªçn model (1-{len(GEMINI_MODELS)}) [default: 1]: ").strip()
        model_idx = int(model_idx) if model_idx else 1
        model_name = GEMINI_MODELS[model_idx - 1] if 1 <= model_idx <= len(GEMINI_MODELS) else GEMINI_MODELS[0]
        
        print(f"\nüîë ƒêang kh·ªüi t·∫°o Gemini v·ªõi auto-fallback...")
        llm_manager = LLMManager(
            provider="gemini",
            model_name=model_name,
            temperature=0.7
        )
        
        print(f"‚úÖ ƒê√£ s·∫µn s√†ng v·ªõi: {llm_manager.model_name}")
        return "1", llm_manager
        
    elif choice == "2":
        # Ollama
        print("\nüí° Nh·∫≠p t√™n Ollama model (v√≠ d·ª•: llama2, mistral)")
        model_name = input("T√™n model [m·∫∑c ƒë·ªãnh: llama2]: ").strip() or "llama2"
        
        llm_manager = LLMManager(
            provider="ollama",
            model_name=model_name,
            temperature=0.7
        )
        
        return "2", llm_manager
        
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, m·∫∑c ƒë·ªãnh d√πng Gemini")
        llm_manager = LLMManager(provider="gemini")
        return "1", llm_manager


def get_gemini_llm(model: str = "gemini-1.5-flash") -> LLMManager:
    """Quick Gemini LLM."""
    return LLMManager(provider="gemini", model_name=model)


def get_ollama_llm(model: str = "llama3:latest") -> LLMManager:
    """Quick Ollama LLM."""
    return LLMManager(provider="ollama", model_name=model)


# Example usage
if __name__ == "__main__":
    print("="*70)
    print("TEST: LangChain LLM Integration")
    print("="*70)
    
    # Test Gemini
    print("\n1. Testing Gemini...")
    try:
        gemini_llm = get_gemini_llm()
        print(f"   Info: {gemini_llm.get_info()}")
        
        response = gemini_llm.generate("Say hello in 5 words")
        print(f"   Response: {response}")
        print("   ‚úÖ Gemini works!")
    except Exception as e:
        print(f"   ‚ùå Gemini failed: {e}")
    
    # Test Ollama (if available)
    print("\n2. Testing Ollama...")
    try:
        ollama_llm = get_ollama_llm()
        print(f"   Info: {ollama_llm.get_info()}")
        
        response = ollama_llm.generate("Say hello in 5 words")
        print(f"   Response: {response}")
        print("   ‚úÖ Ollama works!")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Ollama not available: {e}")
    
    print("\n" + "="*70)
    print("‚úÖ LangChain LLM integration test complete")
