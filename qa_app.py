# qa_app.py

import torch
from sentence_transformers import SentenceTransformer
from milvus import get_or_create_collection
from gemini_client import configure_gemini # Import the configuration function
import os

# --- 1. C·∫§U H√åNH ---
# Import c√°c c·∫•u h√¨nh chung t·ª´ file config.py
from config import EMBEDDING_MODEL_NAME, COLLECTION_NAME

# --- 2. KH·ªûI T·∫†O C√ÅC MODEL ---

def get_embedding_model():
    """
    Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ model embedding, ∆∞u ti√™n s·ª≠ d·ª•ng GPU n·∫øu c√≥.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ü§ñ ƒêang t·∫£i model embedding '{EMBEDDING_MODEL_NAME}' l√™n '{device}'...")
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
        print("   -> ‚úÖ Model embedding ƒë√£ t·∫£i th√†nh c√¥ng!")
        return model
    except Exception as e:
        print(f"   -> ‚ùå L·ªói khi t·∫£i model embedding: {e}")
        return None

# --- 3. H√ÄM T√åM KI·∫æM ---

def search_in_milvus(collection, query_vector, top_k=5):
    """
    T√¨m ki·∫øm c√°c vector t∆∞∆°ng t·ª± trong Milvus.
    """
    print(f"üîç ƒêang t√¨m ki·∫øm {top_k} k·∫øt qu·∫£ li√™n quan trong Milvus...")
    try:
        # ƒê·∫£m b·∫£o collection ƒë√£ ƒë∆∞·ª£c load ƒë·ªÉ t√¨m ki·∫øm
        collection.load() 
        
        search_params = {
            "metric_type": "L2", # Kho·∫£ng c√°ch Euclidean
            "params": {"nprobe": 10},
        }
        
        results = collection.search(
            data=[query_vector],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            output_fields=["text", "page", "pdf_source"] # Y√™u c·∫ßu tr·∫£ v·ªÅ c√°c field n√†y
        )
        
        collection.release() # Gi·∫£i ph√≥ng collection kh·ªèi b·ªô nh·ªõ sau khi t√¨m ki·∫øm
        print("   -> ‚úÖ T√¨m ki·∫øm ho√†n t·∫•t.")
        return results
    except Exception as e:
        print(f"   -> ‚ùå L·ªói khi t√¨m ki·∫øm tr√™n Milvus: {e}")
        return None

def main():
    """
    H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng H·ªèi-ƒê√°p.
    """
    print("--- CH√ÄO M·ª™NG ƒê·∫æN V·ªöI ·ª®NG D·ª§NG H·ªéI-ƒê√ÅP RAG ---")
    
    # --- Kh·ªüi t·∫°o ---
    embedding_model = get_embedding_model()
    # Kh·ªüi t·∫°o model Gemini b·∫±ng h√†m c√≥ s·∫µn
    generative_model = configure_gemini() 
    
    if not embedding_model or not generative_model:
        print("\n--- ·ª®ng d·ª•ng kh√¥ng th·ªÉ kh·ªüi ƒë·ªông do l·ªói. Vui l√≤ng ki·ªÉm tra l·∫°i. ---")
        return

    print("\n--- B∆∞·ªõc 1: K·∫øt n·ªëi t·ªõi Milvus collection ---")
    # recreate=False ƒë·ªÉ kh√¥ng t·∫°o l·∫°i collection
    collection = get_or_create_collection(COLLECTION_NAME, recreate=False)
    if not collection:
        print("   -> ‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi collection. ƒê√£ ch·∫°y file populate_milvus.py ch∆∞a?")
        return
    print(f"   -> ‚úÖ K·∫øt n·ªëi th√†nh c√¥ng t·ªõi collection '{COLLECTION_NAME}'.")
    print(f"   -> Collection hi·ªán c√≥ {collection.num_entities} th·ª±c th·ªÉ.")


    # --- V√≤ng l·∫∑p h·ªèi-ƒë√°p ---
    print("\n--- B·∫Øt ƒë·∫ßu phi√™n h·ªèi-ƒë√°p (g√µ 'exit' ƒë·ªÉ tho√°t) ---")
    while True:
        query = input("\n‚ùì ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n: ")
        if query.lower() == 'exit':
            break
            
        # --- B∆∞·ªõc 2: T·∫°o embedding cho c√¢u h·ªèi ---
        print(f"üß† ƒêang t·∫°o embedding cho c√¢u h·ªèi...")
        query_embedding = embedding_model.encode(query)
        
        # --- B∆∞·ªõc 3: T√¨m ki·∫øm th√¥ng tin li√™n quan (Retrieval) ---
        search_results = search_in_milvus(collection, query_embedding, top_k=5)
        
        if not search_results or not search_results[0]:
            print("   -> ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu.")
            continue
            
        # --- B∆∞·ªõc 4: X√¢y d·ª±ng prompt v√† g·ªçi Gemini (Generation) ---
        print("üìù ƒêang x√¢y d·ª±ng prompt v√† g·ªçi Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi...")
        
        # L·∫•y context t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
        context = ""
        sources = []
        # search_results[0] l√† k·∫øt qu·∫£ cho query ƒë·∫ßu ti√™n (v√† duy nh·∫•t)
        for hit in search_results[0]:
            context += f"- {hit.entity.get('text')}\n"
            sources.append(f"{hit.entity.get('pdf_source')} (Trang {hit.entity.get('page')})")

        # T·∫°o prompt
        prompt = f'''D·ª±a v√†o c√°c th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y t·ª´ m·ªôt t√†i li·ªáu PDF:

{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, kh√¥ng b·ªãa ƒë·∫∑t. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu".

C√¢u h·ªèi: {query}
'''
        
        # G·ªçi Gemini
        try:
            answer = generative_model.generate_content(prompt).text
            print("\n‚úÖ C√¢u tr·∫£ l·ªùi t·ª´ Gemini:")
            print(answer)
            # D√πng set ƒë·ªÉ lo·∫°i b·ªè c√°c ngu·ªìn tr√πng l·∫∑p
            unique_sources = sorted(list(set(sources)))
            print(f"\nNgu·ªìn tham kh·∫£o: {', '.join(unique_sources)}")
        except Exception as e:
            print(f"   -> ‚ùå L·ªói khi g·ªçi Gemini API: {e}")


    print("\n--- C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng! ---")


if __name__ == "__main__":
    main()
