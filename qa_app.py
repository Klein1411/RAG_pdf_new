# qa_app.py

import torch
from sentence_transformers import SentenceTransformer
import os

# --- 1. C·∫§U H√åNH & IMPORT ---
from config import EMBEDDING_MODEL_NAME, COLLECTION_NAME
from milvus import get_or_create_collection
# Import c√°c h√†m x·ª≠ l√Ω LLM t·ª´ file m·ªõi
from llm_handler import initialize_and_select_llm, generate_answer_with_fallback

# --- 2. C√ÅC H√ÄM TI·ªÜN √çCH ---

def get_embedding_model():
    """
    Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ model embedding, ∆∞u ti√™n s·ª≠ d·ª•ng GPU n·∫øu c√≥.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ü§ñ ƒêang t·∫£i model embedding '{EMBEDDING_MODEL_NAME}' l√™n '{device}'...")
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
        print("   -> ‚úÖ Model embedding ƒë√£ t·∫£i th√†nh c√¥ng!")
        return model
    except Exception as e:
        print(f"   -> ‚ùå L·ªói khi t·∫£i model embedding: {e}")
        return None

def search_in_milvus(collection, query_vector, top_k=30):
    """
    T√¨m ki·∫øm c√°c vector t∆∞∆°ng t·ª± trong Milvus.
    """
    print(f"üîç ƒêang t√¨m ki·∫øm {top_k} k·∫øt qu·∫£ li√™n quan trong Milvus...")
    try:
        collection.load()
        search_params = {"metric_type": "L2", "params": {"nprobe": 64}}
        results = collection.search(
            data=[query_vector],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            output_fields=["text", "page", "pdf_source"]
        )
        collection.release()
        print("   -> ‚úÖ T√¨m ki·∫øm ho√†n t·∫•t.")
        return results
    except Exception as e:
        print(f"   -> ‚ùå L·ªói khi t√¨m ki·∫øm tr√™n Milvus: {e}")
        return None

# --- 3. H√ÄM MAIN CH√çNH ---

def main():
    """
    H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng H·ªèi-ƒê√°p.
    """
    print("--- CH√ÄO M·ª™NG ƒê·∫æN V·ªöI ·ª®NG D·ª§NG H·ªéI-ƒê√ÅP RAG Ôºà*Ôºæ3Ôºæ)/~‚òÜ---")
    
    # --- B∆∞·ªõc 1: Kh·ªüi t·∫°o v√† l·ª±a ch·ªçn LLM ---
    # To√†n b·ªô logic ph·ª©c t·∫°p ƒë√£ ƒë∆∞·ª£c chuy·ªÉn sang llm_handler.py
    model_choice, gemini_model, ollama_model_name = initialize_and_select_llm()

    # --- B∆∞·ªõc 2: Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn kh√°c ---
    embedding_model = get_embedding_model()
    if not embedding_model:
        print("\n--- ·ª®ng d·ª•ng kh√¥ng th·ªÉ kh·ªüi ƒë·ªông do l·ªói model embedding. ---")
        return

    print("\n--- B∆∞·ªõc 3: K·∫øt n·ªëi t·ªõi Milvus collection ---")
    collection = get_or_create_collection(COLLECTION_NAME, recreate=False)
    if not collection:
        print("   -> (T_T) Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi collection. ƒê√£ ch·∫°y file populate_milvus.py ch∆∞a?")
        return

    if not collection.has_index() or collection.num_entities == 0:
        print("   -> ‚ö†Ô∏è Collection hi·ªán ƒëang tr·ªëng ho·∫∑c ch∆∞a ƒë∆∞·ª£c ƒë√°nh ch·ªâ m·ª•c.")
        run_populate = input("   -> B·∫°n c√≥ mu·ªën ch·∫°y script ƒë·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu? (y/n): ").strip()
        if run_populate.lower() == 'y':
            from populate_milvus import populate_database
            populate_database()
            print("\n   -> ƒêang t·∫£i l·∫°i collection sau khi ƒë·ªìng b·ªô...")
            collection = get_or_create_collection(COLLECTION_NAME, recreate=False)
        else:
            print("   -> B·ªè qua b∆∞·ªõc ƒë·ªìng b·ªô. ·ª®ng d·ª•ng kh√¥ng th·ªÉ ti·∫øp t·ª•c n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu.")
            return

    print(f"\n--- B∆∞·ªõc 4: T·∫£i collection '{COLLECTION_NAME}' v√†o b·ªô nh·ªõ ---")
    collection.load()
    print(f"   -> (¬∞o¬∞) Collection ƒë√£ s·∫µn s√†ng! Hi·ªán c√≥ {collection.num_entities} th·ª±c th·ªÉ.")

    # --- V√≤ng l·∫∑p h·ªèi-ƒë√°p ---
    print("\n--- B·∫Øt ƒë·∫ßu phi√™n h·ªèi-ƒë√°p (g√µ 'exit' ƒë·ªÉ tho√°t :3) ---")
    while True:
        query = input("\n‚ùì ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n: ")
        if query.lower() == 'exit':
            break
            
        print(f"üß† ƒêang t·∫°o embedding cho c√¢u h·ªèi...")
        query_embedding = embedding_model.encode(query)

        search_results = search_in_milvus(collection, query_embedding, top_k=30)

        if not search_results or not search_results[0]:
            print("   -> ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu.")
            continue
            
        print("üìù ƒêang x√¢y d·ª±ng prompt ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi...")
        
        context = ""
        sources = []
        for hit in search_results[0]:
            context += f"- {hit.entity.get('text')}\n"
            sources.append(f"{hit.entity.get('pdf_source')} (Trang {hit.entity.get('page')})")

        # # DEBUG: In ra context v√† sources ƒë·ªÉ ki·ªÉm tra
        # print("\n---------------- DEBUG: CONTEXT TRUY XU·∫§T ----------------")
        # print("Ng·ªØ c·∫£nh ƒë∆∞·ª£c l·∫•y t·ª´ Milvus:")
        # print(context)
        # unique_sources_for_debug = sorted(list(set(sources)))
        # print(f"Ngu·ªìn tham kh·∫£o (tr∆∞·ªõc khi ƒë∆∞a v√†o LLM): {', '.join(unique_sources_for_debug)}")
        # print("----------------------------------------------------------\n")

        prompt = f'''D·ª±a v√†o c√°c th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y t·ª´ m·ªôt t√†i li·ªáu PDF:\n\n{context}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, kh√¥ng b·ªãa ƒë·∫∑t. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu".\n\nC√¢u h·ªèi: {query}\n'''
        
        # --- G·ªçi model v·ªõi logic retry v√† fallback ---
        answer = generate_answer_with_fallback(prompt, model_choice, gemini_model, ollama_model_name)

        print("\n‚úÖ C√¢u tr·∫£ l·ªùi:")
        print(answer)
        
        if not "[L·ªñI H·ªÜ TH·ªêNG]" in answer:
            unique_sources = sorted(list(set(sources)))
            print(f"\nNgu·ªìn tham kh·∫£o: {', '.join(unique_sources)}")

    print("\n--- C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng! ---")


if __name__ == "__main__":
    main()