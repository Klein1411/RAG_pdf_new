# llm_handler.py

import time
import requests

# S·ª≠ d·ª•ng GeminiClient m·ªõi, m·ªôt class qu·∫£n l√Ω stateful
from gemini_client import GeminiClient
from config import OLLAMA_API_URL, OLLAMA_MODELS, GEMINI_INPUT_TOKEN_LIMIT
from logging_config import get_logger

logger = get_logger(__name__)

# --- C·∫§U H√åNH ---
MAX_RETRIES = 3
RETRY_DELAY = 2 # Gi√¢y

# --- C√ÅC H√ÄM G·ªåI MODEL (C√ì X·ª¨ L√ù L·ªñI) ---

def call_ollama(prompt: str, model_name: str) -> str:
    """
    G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama local API. N√©m ra Exception n·∫øu c√≥ l·ªói.
    """
    logger.info(f"G·ª≠i y√™u c·∫ßu ƒë·∫øn model Ollama '{model_name}'")
    print(f"   -> üí¨ ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn model local '{model_name}' qua Ollama...")
    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False
    }
    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        response_data = response.json()
        result = response_data.get("response", "").strip()
        logger.info(f"Nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Ollama (ƒë·ªô d√†i: {len(result)} k√Ω t·ª±)")
        return result
    except requests.exceptions.ConnectionError as e:
        logger.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama t·∫°i {OLLAMA_API_URL}")
        raise ConnectionError(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama t·∫°i {OLLAMA_API_URL}. B·∫°n ƒë√£ b·∫≠t Ollama ch∆∞a?") from e
    except requests.exceptions.RequestException as e:
        if e.response and e.response.status_code == 404:
            logger.error(f"Endpoint Ollama kh√¥ng t·ªìn t·∫°i: {OLLAMA_API_URL}")
            raise FileNotFoundError(f"L·ªói 404: Endpoint '{OLLAMA_API_URL}' kh√¥ng t·ªìn t·∫°i. URL trong config.py c√≥ th·ªÉ sai.") from e
        logger.error(f"L·ªói khi g·ªçi Ollama API: {e}")
        raise IOError(f"L·ªói khi g·ªçi Ollama API: {e}") from e

def call_gemini(prompt: str, gemini_client: GeminiClient) -> str:
    """
    G·ª≠i y√™u c·∫ßu ƒë·∫øn Gemini API th√¥ng qua GeminiClient.
    Client s·∫Ω t·ª± ƒë·ªông qu·∫£n l√Ω xoay v√≤ng key.
    """
    if not gemini_client:
        logger.error("Gemini client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        raise ValueError("Gemini client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")

    # Ki·ªÉm tra token ch·ªß ƒë·ªông
    try:
        logger.info("ƒê·∫øm token cho prompt")
        print(f"   -> ‚ÑπÔ∏è ƒêang ƒë·∫øm token cho prompt...")
        token_count = gemini_client.count_tokens(prompt).total_tokens
        logger.info(f"Prompt c√≥ {token_count} tokens")
        print(f"   -> ‚ÑπÔ∏è ∆Ø·ªõc t√≠nh prompt c√≥ {token_count} tokens.")
        if token_count > GEMINI_INPUT_TOKEN_LIMIT:
            logger.error(f"Prompt qu√° l·ªõn: {token_count} tokens > {GEMINI_INPUT_TOKEN_LIMIT}")
            raise ValueError(f"Prompt qu√° l·ªõn ({token_count} tokens), v∆∞·ª£t ng∆∞·ª°ng an to√†n {GEMINI_INPUT_TOKEN_LIMIT} tokens.")
    except Exception as e:
        raise e

    # generate_content c·ªßa client ƒë√£ bao g·ªìm logic xoay v√≤ng key
    logger.info("G·ª≠i y√™u c·∫ßu t·ªõi Gemini API")
    result = gemini_client.generate_content(prompt)
    logger.info(f"Nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini (ƒë·ªô d√†i: {len(result)} k√Ω t·ª±)")
    return result

# --- H√ÄM LOGIC CH√çNH (RETRY & FALLBACK) ---

def generate_answer_with_fallback(prompt, model_choice, gemini_client, ollama_model_name):
    """
    H√†m ch√≠nh ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi, v·ªõi logic fallback ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.
    """
    primary_func, primary_name, fallback_func, fallback_name = (None, None, None, None)
    answer = None

    if model_choice == '1':
        primary_func = lambda: call_gemini(prompt, gemini_client)
        primary_name = "Gemini"
        fallback_func = lambda: call_ollama(prompt, ollama_model_name)
        fallback_name = "Ollama"
    else:
        primary_func = lambda: call_ollama(prompt, ollama_model_name)
        primary_name = "Ollama"
        fallback_func = lambda: call_gemini(prompt, gemini_client)
        fallback_name = "Gemini"

    # --- Th·ª≠ model ch√≠nh ---
    logger.info(f"Th·ª≠ model ch√≠nh: {primary_name}")
    print(f"\n‚ñ∂Ô∏è ƒêang th·ª≠ model ch√≠nh: {primary_name}...")
    try:
        answer = primary_func()
        logger.info(f"Model ch√≠nh {primary_name} th√†nh c√¥ng")
        return answer
    except Exception as e:
        # C√°c l·ªói nh∆∞ h·∫øt key, prompt qu√° l·ªõn, connection error s·∫Ω ƒë∆∞·ª£c b·∫Øt ·ªü ƒë√¢y
        logger.error(f"Model ch√≠nh ({primary_name}) th·∫•t b·∫°i: {e}")
        print(f"   -> ‚ö†Ô∏è Model ch√≠nh ({primary_name}) th·∫•t b·∫°i ho√†n to√†n: {e}")

    # --- Th·ª≠ model d·ª± ph√≤ng ---
    logger.info(f"Chuy·ªÉn sang model d·ª± ph√≤ng: {fallback_name}")
    print(f"\n‚Ü™Ô∏è Chuy·ªÉn sang model d·ª± ph√≤ng: {fallback_name}...")
    try:
        answer = fallback_func()
        logger.info(f"Model d·ª± ph√≤ng {fallback_name} th√†nh c√¥ng")
        return answer
    except Exception as e:
        logger.error(f"Model d·ª± ph√≤ng ({fallback_name}) th·∫•t b·∫°i: {e}")
        print(f"   -> ‚ùå Model d·ª± ph√≤ng ({fallback_name}) c≈©ng th·∫•t b·∫°i: {e}")

    logger.critical("C·∫£ hai model ch√≠nh v√† d·ª± ph√≤ng ƒë·ªÅu th·∫•t b·∫°i")
    return "[L·ªñI H·ªÜ TH·ªêNG] C·∫£ hai model ch√≠nh v√† d·ª± ph√≤ng ƒë·ªÅu kh√¥ng th·ªÉ ph·∫£n h·ªìi."


# --- H√ÄM KH·ªûI T·∫†O V√Ä L·ª∞A CH·ªåN ---

def initialize_and_select_llm():
    """
    X·ª≠ l√Ω vi·ªác c·∫•u h√¨nh v√† l·ª±a ch·ªçn model c·ªßa ng∆∞·ªùi d√πng.
    Tr·∫£ v·ªÅ: (model_choice, gemini_client, ollama_model_name)
    """
    logger.info("B·∫Øt ƒë·∫ßu c·∫•u h√¨nh model sinh c√¢u tr·∫£ l·ªùi")
    print("\n‚ú® ƒêang c·∫•u h√¨nh c√°c model sinh c√¢u tr·∫£ l·ªùi...")
    gemini_client = None
    try:
        gemini_client = GeminiClient()
        logger.info("Kh·ªüi t·∫°o Gemini Client th√†nh c√¥ng")
        print("   -> ‚úÖ Gemini ƒë√£ s·∫µn s√†ng (v·ªõi tr√¨nh qu·∫£n l√Ω API key).")
    except Exception as e:
        logger.warning(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini Client: {e}")
        print(f"   -> ‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini Client: {e}. N√≥ s·∫Ω kh√¥ng kh·∫£ d·ª•ng.")

    ollama_model_name = ""
    model_choice = ""
    
    while True:
        print("\nVui l√≤ng ch·ªçn model CH√çNH ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi:")
        prompt_text = "   1: Gemini (Cloud API)\n   2: Ollama (Local)\nL·ª±a ch·ªçn c·ªßa b·∫°n: "
        model_choice = input(prompt_text).strip()
        
        if model_choice == '1':
            if gemini_client:
                logger.info("Ng∆∞·ªùi d√πng ch·ªçn Gemini l√†m model ch√≠nh")
                print("   -> ‚úÖ Model ch√≠nh l√† Gemini.")
                return model_choice, gemini_client, ollama_model_name
            else:
                logger.warning("Gemini ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, y√™u c·∫ßu ch·ªçn Ollama")
                print("   -> ‚ùå Gemini ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, vui l√≤ng ch·ªçn Ollama.")
        elif model_choice == '2':
            if not OLLAMA_MODELS:
                logger.warning("Danh s√°ch model Ollama trong config.py ƒëang tr·ªëng")
                print("   -> ‚ùå Danh s√°ch model Ollama trong config.py ƒëang tr·ªëng.")
                continue

            print("   -> Vui l√≤ng ch·ªçn model Ollama:")
            for i, model in enumerate(OLLAMA_MODELS):
                default_text = " (m·∫∑c ƒë·ªãnh)" if i == 0 else ""
                print(f"      {i+1}: {model}{default_text}")

            while True:
                choice = input(f"   -> L·ª±a ch·ªçn c·ªßa b·∫°n (nh·∫•n Enter ƒë·ªÉ ch·ªçn m·∫∑c ƒë·ªãnh): ").strip()
                if not choice:
                    ollama_model_name = OLLAMA_MODELS[0]
                    break
                try:
                    choice_idx = int(choice) - 1
                    if 0 <= choice_idx < len(OLLAMA_MODELS):
                        ollama_model_name = OLLAMA_MODELS[choice_idx]
                        break
                    else:
                        print("      -> L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")
                except ValueError:
                    print("      -> Vui l√≤ng nh·∫≠p m·ªôt s·ªë.")
            
            logger.info(f"Ng∆∞·ªùi d√πng ch·ªçn model Ollama: {ollama_model_name}")
            print(f"   -> ‚úÖ Model ch√≠nh l√† '{ollama_model_name}' t·ª´ Ollama.")
            return model_choice, gemini_client, ollama_model_name
        else:
            logger.warning(f"L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá: {model_choice}")
            print("   -> L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p 1 ho·∫∑c 2.")